---
title: |
  \large \textbf{Title:} Computational models reveal how cleaner fish adjust decisions in a biological market
subtitle: |
  \normalsize \textbf{Abbreviated title:} Cleaner fish cognitive mechanisms in a biological market
output:
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
    template: template.latex
  word_document: default
fig_caption: yes
fontsize: 12pt
keep_tex: yes
bibliography: Cleanerlearning.bib
style: "M:\\Zotero\\styles\\behavioral-ecology.csl"
spacing: double
abstract: |
  While it is generally straightforward to quantify individual performance
  in cognitive experiments, identifying the underlying cognitive processes remains
  a major challenge. Often, different mechanistic underpinnings yield similar performances,
  and Lloyd Morgan’s cannon warrants acceptance of the simpler explanation. Alternatively,
  when the different mechanisms interact with environmental conditions, variation
  in performance across environments might allow to statistically infer the mechanism
  responsible. We illustrate this point by fitting computational models to experimental
  data on performance by wild-caught cleaner fish *Labroides dimidiatus* in an ephemeral
  reward task, as well as cleaner and client fish densities from the locations of
  capture. Using Bayesian statistics to fit the model parameters to performance data
  revealed that cleaner fish most likely estimate future consequences of an action,
  while it appears unlikely that the removal of the ephemeral reward acts as psychological
  punishment (negative reinforcement). Incorporating future consequences also yields
  performances that can be considered the result of locally optimal decision-rules,
  in contrast to the negative reinforcement mechanism. We argue that the combination
  of computational models with data is a powerful tool to infer the mechanistic underpinnings
  of cognitive performance.
lay_summary:  |
  Cleaner fish eat ectoparasites off other fishes, so-called clients.
  It regularly happens that two clients seek a cleaner's service simulatenously. Cleaners
  benefit from prioritising clients unwilling to wait, so they can feed on those willing
  to wait. To make the right choice, cleaners must somehow “look” into the future
  to anticipate consequences of current choices. By combining a learning model with
  data, we show that cleaners estimate the long-term value of their actions rather
  than using simpler heuristics. Estimating long-term value is a mechanism involved
  in human foresight.
header-includes: |
  ```{=latex}
    \usepackage{float}
    \usepackage{booktabs}
    \newcommand{\beginsupplement}{ \setcounter{table}{0}             \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
  ```
editor_options:
  markdown:
    wrap: 80
---

<!-- ```{=latex} -->
<!-- \keywords{learning, behavior, cleaners, bayesian statisitics, behavioral mechanisms} -->
<!-- ``` -->

\newpage

## Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE,message=FALSE,cache = TRUE)
library(here)
source(file=here("loadFieldData.R"))
```

Often alternative cognitive mechanisms yield similar behavior and/or cognitive
performances. This poses a problem for disentangling the mechanistic 
underpinnings of behavior. This is particularly clear in research aimed at
discovering between species variation in *higher* cognitive abilities; or 
in other words, research on whether non-human animals show cognitive 
abilities believed to be uniquely human. For instance, when researchers 
try to find *mental time travel-like* behavior, they usually come up with 
experiments to show the behavior displayed requires inferences made 
through past events [@dally_Foodcaching_2006]. However, they often face the
challenge of alternative scenarios where simpler explanations, 
like classic associative learning, can bring about the 
observed behavioral outcome [@suddendorf_Evolution_2007]. Similarly, attempts 
to demonstrate the presence of *theory of mind* in non-human animals 
face objections justified by alternative mechanisms underpinning 
similar behavioral results [@heyes_Theory_1998]. 
Such controversies are usually settled by using the principle 
of parsimony and its cognitive version, 
Lloyd Morgan’s cannon, which states that the simpler explanation (mechanism)
should be accepted. Ideally, alternative hypotheses should be evaluated 
in light of their explanatory power. 

<!-- If alternative mechanisms predict different  -->
<!-- outcomes when exposed to different contexts, the variation arising from -->
<!-- varying the context could open up a way to infer which mechanism is used.  -->
<!-- Controlled laboratory conditions often mask between and within  -->
<!-- species variation that arises among different environmental contexts. Thus, if  -->
<!-- predictions from the alternative mechanisms depend on environmental  -->
<!-- conditions, it is possible to assess their relative explanatory power  -->
<!-- by performing cognitive tests in the context of different  -->
<!-- environmental conditions.  -->

Learning is a key overarching cognitive mechanism that allows 
individuals to associate rewards with environmental stimuli 
and thus behave adaptively [@staddon_Adaptive_2016;@shettleworth_Cognition_2009]. 
Associative learning, in particular, exists in all major vertebrate taxa, 
and in many invertebrates as well [@heyes_Simple_2012;@macphail_Brain_1982; 
@staddon_Adaptive_2016; @behrens_Associative_2008]. 
Associative learning is not homogeneous throughout its taxonomic distribution, 
rather there are differences across and within species 
[@shettleworth_Cognition_2009;@sih_Linking_2012]. So presumably, 
the mechanistic underpinnings of learning have been modified by 
natural selection [@marler_Species_1989]. 

One way to formalize the alternative mechanistic underpinnings of 
associative learning is to develop quantitative models of learning 
processes. This approach, which started within experimental psychology 
[@staddon_Adaptive_2016;@brandon_Computational_2002], has been very fruitful 
in disentangling the mechanistic structure of cognitive systems. 
More recently, the development of reinforcement learning theory 
[@sutton_Reinforcement_2018], has allowed to 
evaluate the empirical support for alternative mechanistic hypotheses by
providing quantitative predictions which are amenable to statistical tests
[@farashahi_Learning_2020;@farashahi_Featurebased_2017]. Interestingly, these 
learning models not only have received support from behavioral data, but also 
are consistent with the current view on reward processing in the brain 
[@schultz_Neuronal_2015].


From an evolutionary perspective, mechanisms are likely selected because 
of how they allow individuals to respond to environmental variation. 
For example, biological market theory predicts that the exchange rate of goods 
and/or services traded between cooperative partners adjusts to the 
law of supply and demand, when individuals have some degree of partner choice
[@noe_Biological_1995a]. Supply and demand conditions, which typically 
depend on the abundance of the species involved, certainly vary in time 
and space. Therefore, natural selection should favor the ability 
to flexibly adjust decisions and behavioral output to 
current market conditions. Indeed, such adjustments have been 
documented [@axen_Signalling_1996]. One example of strategic 
adjustment in a biological market is the marine
cleaning mutualism involving the cleaner fish *Labroides dimidiatus* and
'client' fishes. Client fishes seek cleaner fish services at their territory 
(so-called “cleaning station”) and offer themselves as food patches 
to get their ectoparasites removed, which provides cleaners 
with food and clients with improved health [@waldie_LongTerm_2011;
@ros_Does_2011;@triki_Effects_2016;@demaire_Reduced_2020]. 
Given the capacity of some client fish to swim larger distances and 
access multiple cleaning stations while others access the only cleaning 
station in their territory, it is crucial to categorize clients as 
either “visitors” or “residents”, respectively. During cleaning interactions, 
a cleaner fish often faces a choice between a visitor and a resident client 
seeking its cleaning services simultaneously. Visitors have the option 
to switch to another cleaner fish if being made to wait, while residents 
must wait for inspection. Indeed,
visitors have been observed to use their partner choice option in that
way [@bshary_Choosy_2002], which may explain why cleaners give
visitors service priority in a field study in the Red Sea
[@bshary_Cleaner_2001a]. Furthermore, in a lab based paradigm, design 
to mimic the resident-visitor choice (ephemeral reward task), cleaners learned 
to prefer the cue associated with the epheral food source (visitor) and
hence accessed both food sources, obtaining 
double the amount of food [@bshary_Asymmetric_2002].
However, further exploration revealed that not all cleaner fish 
manage to develop a preference for the ephemeral option 
in the lab [@triki_Decrease_2018]. Over the last decade, over a hundred 
wild-caught cleaner fish have been tested
in the exact same paradigm of the ephemeral reward 
task [@salwiczek_Adult_2012;@wismer_Variation_2014;@triki_Decrease_2018;
@triki_Biological_2019;@triki_Brain_2020]. These fish often come from 
different reef locations. Investigation of the local 
eco-sociological conditions revealed that cleaner and client 
fish population densities have a substantial impact on cleaner fish 
performance in the task. Cleaner fish from reef sites with relatively 
low densities were more likely to fail at solving the task 
[@triki_Biological_2019;
@triki_Decrease_2018; @wismer_Variation_2014]. This intra-specific variation
is unlikely due to local genetic adaptation, because cleaner
fish are open water spawners and the environmental conditions can vary within 
the lifespan of a fish. 


Mechanistic models explicitly designed to mimic the ephemeral reward task 
have shown that the simplest form of associative learning 
(operant conditioning) cannot account for a solution to 
the ephemeral reward task [@prat_Modelling_2022;
@quinones_Reinforcement_2019]. Operant conditioning is 
a form of associative learning where individuals use short term 
reward to associate and choose actions. Such models allow varying 
the cognitive tool kit and evaluating which minimal kit is necessary 
to solve the task at hand (e.g. @dubois_Model_2021). 
To be able to give visitors priority over residents, 
cleaners need to be able to assess a client’s value separately for the 
three possible scenarios (alone, paired with a fish with the 
same strategic option, paired with a fish with the alternative strategic option) 
[@quinones_Reinforcement_2019]. The ability to distinguish and value 
one stimulus differently alone from compound versions of it has been termed 
configurational learning, chunking, or 
segmentation (see references in @prat_Modelling_2022). 
In addition to configurational learning, cleaners also need to account for 
the future consequences of current decisions. In the model by 
@quinones_Reinforcement_2019, this could be achieved in 
two non-mutually exclusive ways: through low temporal discounting of 
future effects, also termed ‘chaining’ [@enquist_Power_2016]; and/or 
through perceiving a visitor client leaving as psychological punishment 
(i.e. as a negative reinforcer). Chaining is when 
individuals include in their valuation of an action the reward effects 
that this will have in the future. This is done by combining in a single 
valuation the reward obtained in the current time with all the reward that 
comes after, discounting for how far in the future reward 
is accrued. 'Chaining' the reward of these different time steps allows 
individuals to take actions that increase the long-term reward at the sacrifice
of short term considerations [@enquist_Power_2016]. Even though, 'chaining' 
can be readily implemented computationally in learning models 
[@enquist_Power_2016;@sutton_Reinforcement_2018], cognitively it seems to be 
a complex adaptation [@suddendorf_Evolution_2007]. On the other hand,
using client behavior as a negative reinforcer is, in principle, 
easier to implement. Thus, the standard logic of 
Lloyd Morgan’s cannon demands that operant conditioning as the simpler 
explanation is to be accepted by default. Ideally, however, the two 
mechanisms should be evaluated in light of how well 
they explain the available data. Note that different fields interested in
cognition and decision making use different words to refer to negative 
reinforcers [@quinones_Reinforcement_2019;@sutton_Reinforcement_2018]. 
Here, for the sake of simplicity and clarity, we will use the word 
'penalty' to refer to this mechanism which includes a negative reinforcer. 


In here we used the field and experimental data to fit the parameters of 
a reinforcement learning model to infer  the 
cognitive mechanism that cleaners use in their interaction with clients. 
Specifically, our approach of fitting the computational model 
to the empirical data aimed at: (i), determining which 
mechanism cleaner fish use to incorporate future consequences of current 
decisions by testing whether chaining, penalty, or a combination 
of both best explains their performance; (ii)  determining whether the 
two mechanisms differed with respect to the ecological conditions 
that are likely to cause high versus low performance in the 
ephemeral reward task. Additionally, we assessed which mechanism 
yields optimal performance patterns. Relying on the logic of 
biological market theory, we predicted that appropriate performance 
is to show a high preference for visitors only under high
local cleaner-to-client ratio.

## Methods


### The model

The model consists of a set of individual-based simulations where
individuals, representing cleaner fish, face a series of choices 
between two options, which simulate the natural conditions 
of the cleaning market. Individuals experience a series 
of discrete time points in which they face different ‘states’, 
defined by the number and category of client fish (visitor or residents) 
inviting for cleaning services. There are six possible states: 
zero clients, one resident, one visitor, 
resident-resident, visitor-visitor, and resident-visitor. 
The probability of each state is largely determined by the relative 
abundance of cleaner fish, residents and visitors, but to some degree by
cleaner fish choices when it faces the resident-visitor combination. 
This is because residents are willing to queue for cleaning service; 
while visitors leave the queue (with a certain probability) when made to wait. 
Individuals obtain a fixed reward from cleaning a client fish
regardless of the category. Every time individuals face and make a choice
they update the probability of making that same choice. The update is
based on the difference between the expected value and the obtained
reward - the prediction error ($\delta_t$) - [@sutton_Reinforcement_2018; 
@rescorla_Theory_1972]. Formally, the prediction error is given by

\begin{equation}
\delta_t = R_t -  V_t(S_t) + \gamma V_t(S_{t+1}) ,
\label{eq:delta}
\end{equation}

where $R_t$ is the sum different reward sources at time $t$; 
$V_t(S_t)$ is the estimated value at time $t$ of the the state faced by the agent 
at time $t$; similarly $V_t(S_{t+1})$ is the estimated value of the state
to come in the following time-step, $\gamma$ is the discount factor for
future rewards. When the estimated value of the current state ($V_t(S_t)$) 
is equal to the sum of short-term ($R_t$) and future discounted reward 
($\gamma V_t(S_{t+1})$) learning stops for that state. If $\gamma = 0$ 
the estimates made by the agent only capture short-term reward. We assume 
short-term reward to two components: positive reward determined by the 
amount of food obtained from cleaning a client; and negative reward triggered
when by a client leaving the station without being cleaned. Formally, 
we let total reward be given by $R_t=P_t - \eta_t$. Where $\eta$ is a parameter
of the model that determines the the size of the negative reward triggered by
unattended clients leaving the station.

The prediction error (Eq. \ref{eq:delta}) is used to update the value of each one
of the states the agent faces, as well as the preference for the 
resident and visitor options. The value update is simple the product of the 
prediction error and the parameter for the speed of learning 
($\Delta V(S_t)=\alpha \delta_t$). The change in preference between the resident 
and visitor is given by $\Delta (\theta_v-\theta_r)_t=\alpha\delta_t2(1-\pi_v)$,
where $\theta_i$ represent the preference for one of two options and the 
difference captures the total change relative to one another; 
$\pi_v$ corresponds to the current probability of choosing the visitor. 
$p_i$ is determined by applying the logistic function to the difference in
preferences between the two mutually exclusive options 
($\pi_v=\frac{1}{1+e^{-(\theta_v-\theta_r)}}$). This amounts
to a preference update that is carried in the direction that 
leads to more reward being obtained, given
the new information. In the long run, the probability of choosing a visitor 
over a resident converges in the model. To which probability the model 
will converge depends on the relative abundance of cleaners, visitors 
and residents; as well as on the probability of visitors leaving the 
cleaning station when made to wait. Further details of the model 
implementation can be found in @quinones_Reinforcement_2019. 

The model shows that agents need to find a way 
to incorporate future consequences of current choices. In the model, 
this could be achieved with either of two parameters that could 
also work together.  First, $\gamma$ measures how
much individuals include future rewards in their decision updates. 
If $\gamma=0$, individuals only use the immediate reward obtained from a
cleaning interaction. As $\gamma$ increases, individuals include more of the
reward obtained from the subsequent choices. That amounts to estimating
and using for decision making the future expected rewards of an action (chaining).
Second, $\eta$ measures how much individuals include in their reward the
fleeing behavior of visitors as a negative component (penalty). Both of
these parameters allow individuals to use in their estimates the future
effects of their choices. 


### Empirical data

The empirical data were collected between 2010 and 2019 always during 
the austral winter months June to August from a total of five 
study reef sites (Corner Beach-CB, Horseshoe-HS, Mermaid Cove-MC, 
Northern Horseshoe-NHS, and The Crest-TC)  at Lizard Island 
($14.6682^\circ S, 145.4604^\circ E$), Great Barrier Reef, Australia. 
The data consist of three sets: fish censuses, field 
observations of cleaner-client interactions to quantify the probability 
of visitors leaving if made to wait, and the 
performance of wild-caught cleaner fish in the ephemeral reward test. 
In total, we have twelve site/year data sets for fish censuses and 
corresponding performance in the lab test. Thus, some sites were sampled more 
than once. To estimate the population density of cleaner fish 
and their clients at a given site in 
a given year, @triki_Biological_2019 used a series of ten 
transects of $30m$ each. Observers swam along the transect lines placed on 
the reef and first counted the visible large-bodied adult fish 
(species with total length $TL \geq 10cm$) including cleaner fish 
on a width of $5m$, and then on the 
return individuals of small-bodied fish species ($TL < 10 cm$)
on a width of $1 m$
(see @triki_Decrease_2018 for further details on fish censuses data collection). 
Total length estimates were done by the observer. 
We then scaled the counts of cleaner fish, small-bodied, 
and  large-bodied  clients fish densities per $100 m^2$. 


The field observation data consisted of video recordings/encodings of the 
cleaner-client cleaning interactions. There were videos from eight 
cleaners per site/year of a duration of $30 min$ each. Triki et al. 
extracted information from every event wherein a visitor client 
was made to wait in favor of another client (visitor or resident), and noted 
whether or not the visitor left or queued for the cleaning service
[@triki_Biological_2019;@triki_Brain_2020]. 

The cognitive performance data was from a total of 120 cleaners 
(10 individuals per 12 site/year) tested in the 
ephemeral reward task [@triki_Biological_2019;@triki_Brain_2020]. Authors 
housed all captured cleaners individually in glass aquaria 
($62cm \times 27cm \times 37 cm$) and provided them 
with PVC pipes ($10 cm \times 1 cm$) as shelters. 
The task consisted of exposing the cleaner fish to substitute 
models of client fish in the form of two *Plexiglas* plates offering the 
same amount of food (one item of mashed prawn).  The two plates differed 
in colour and pattern (horizontal green stripes or vertical pink stripes) 
but had equal size ($10 cm \times 7 cm$). Importantly, the two plates played 
different roles as either a visitor (ephemeral food source) or 
resident (permanent food source). That is, if a cleaner fish inspected the 
resident plate first, the experimenter withdrew the visitor plate out of 
the aquarium as a consequence.  Choosing first the visitor plate, 
however, granted access to both plates. The equal size of the plates 
forced cleaner fish to decide based solely on the association between 
the behaviour and the color/pattern cue [@wismer_Cuebased_2019]. 
Triki et al. [-@triki_Biological_2019;@triki_Brain_2020] 
tested the fish for a maximum of 200 trials with 20 trials a day, 10 trials 
in the morning and 10 trials in the afternoon. They randomized and 
counterbalanced the plates’ spatial location (i.e. left or right) 
between trials. Similarly, they counterbalanced the plates’ decoration 
(colour and pattern) and the plates’ role (visitor or resident) between the 
tested fish. In the original studies, once a fish reached a learning criterion, 
that is, performing significantly above chance level in a binomial test 
($p-value 	\leq 0.05$, ),
they passed to a reversal version of the task where the roles of the 
visitor/resident Plexiglas plates were swapped. The reversal phased 
stopped when the fish performed significatly above chance, or the fish
completed the 200 trial together with the initial;
see Triki et al.[-@triki_Biological_2019;@triki_Brain_2020]. 
Here, we are interested in explaining the total frequency of visitor choices
using the model, rather than just the achievement of the criterion. 
Total frequency of visitor choices naturally comes out of the model, 
and allows us to use all the variation among cleaners, instead of reducing 
that to a binomial variable.
Thus, we used instead a subset of these data to estimate the final 
cleaner fish preferences for the visitor plate, even if they do not reach 
the learning criterion within 200 trials. To do so, we first extracted the 
trial-by-trial outcomes from the last two sessions (20 trials) of those who 
never reached the learning criterion for visitor plate 
($N = `r fieldData.cleaner.filt.sum[V1==2,length(V1)]`$ cleaner fish). 
For those who reached the learning criterion at some point during the 
test and passed to a reversal phase, we extracted the trial-by-trial 
outcomes from the last session (10 trials) before passing to reversal 
and the last 10 trials they were exposed to in the test 
($N = `r fieldData.cleaner.filt.sum[V1>2,length(V1)]`$ cleaner fish). 

We chose a combination of initial and reversal to quantify preference 
for the visitor client. However, it could be argued that using only the initial
phase gives a better estimation of the cleaner fish preference for the visitor. 
In the supplementary material (Fig. \ref{fig:rawdata}) we show how using 
initial and reversal (a), and only initial (b) maps
to the previously used criteria. The initial and reversal match better
the criteria chosen in previous analysis of the ephemeral 
reward task experimental set-up
[@triki_Biological_2019;@triki_Brain_2020]. 

### Statistical analysis

The aim of the analyses is to fit the key model parameters 
$\gamma$ and $\eta$, to the empirical data from Triki et al. 
[-@triki_Biological_2019;@triki_Brain_2020] to test whether each or 
a combination of these effects is a better explanation for the pattern 
seen in the data. We used the ecological variables: cleaners, visitor clients, 
resident clients abundances and visitor clients leaving probability,  
as input to the models. As the response variable, we used the frequency with 
which cleaners chose the visitor option in the ephemeral reward task. 
Finally, we used the probability with which agents in the model
simulations choose the visitor in resident-visitor options
as the prediction for the response variable. 
We kept all other parameter values used for 
the model simulations constant, see Table \ref{tab:param}.

To capture with the model the relationship between the ecological 
variables and cleaner fish preferences for visitors, we needed to scale 
the absolute population densities of cleaner fish from the empirical data
to a measure of relative abundances that captures client visitation patterns. 
This is because, in the model, relative abundances of clients 
define not only the probability of residents and visitors 
but also how often the cleaning station is empty (e.g. there are no 
clients to be cleaned). The frequency with which clients visit the station 
is another variable influencing the station occupancy, which in nature 
may vary among different client species 
depending on their ectoparasite loads. We do not have field estimates 
for species-specific parasite loads, especially not as a function of the site. 
In order to control for these aspects, we computed a measure of relative 
cleaner fish abundance for each reef site relative to 
absolute abundances and multiplied it
by a scaling constant that changes the range of the variable. Specifically,
cleaner fish relative abundance is equal 
to $\frac{\epsilon C_{abs}}{\epsilon C_{abs}+R_{abs}+V_{abs}}$. 
Where $\epsilon$ is the scaling constant; $C_{abs}$ 
is the cleaner fish absolute density; $R_{abs}$ is the residents absolute 
density and $V_{abs}$ is the visitor client absolute density. This scaling 
constant is meant to capture variation in the market conditions driven 
partly by cleaner fish abundance. We fitted the value of the scaling constant
($\epsilon$) as part of the statistical inference. 
As for the visitor and resident abundances, we computed a 
relative measure with respect to the 
total client abundance, and weighted that by the re-scaled relative absence 
of cleaners ($1-\text{relative cleaner abundance}$). Thus, all three measures 
of relative abundance sum up to 1, and can be used in the model as a proxy 
for the probability of having different options in the cleaning station. 
Note that we introduced the scaling constant to control for variation that 
is not captured by the model; its parameter distribution does not offer 
biological insights.

Once we calculated the relative abundances, we obtained predictions from 
the model for each one of the locations and ran the Markov Chain. We started the
chain with random values for the parameters of interest, then ran the 
computational model once for every reef site using as input the ecological 
explanatory variables. The model outputs the probability of choosing the visitor
for each location $p_i$, where $i$ is the index for the 12 locations. Assuming a 
binomial distribution, the probability that each of the cleaner fish 20 choices 
in the ephemeral reward task was generated by the model 
is given by $\binom{n}{k_ij}p^{k_{ij}}_i (1-p_i)^{n-k_{ij}}$;
where $k_{ij}$ is the number of times that cleaner fish $j$ in reef site $i$ 
chose the visitor over the resident; and $n=20$ (due to our choice of using 20 
choices per cleaner fish). By taking the natural logarithm and summing over all 
the cleaner fish and reef sites, we obtained the log-likelihood of the data 
given the model and parameters. We then proposed a new set of parameters drawn 
from a uniform distribution centered around the old parameter set. The amplitude 
of the uniform distribution used for each parameter can be found in Table 
\ref{tab:param}. Subsequently, we ran the model and calculated the likelihood 
with the new parameter set. We then used the ratio of the two likelihoods to 
choose which parameter set to keep. New parameter sets with a higher likelihood 
than the old set replaced old ones, and those with a lower likelihood replaced 
current ones  with a probability equal to the log-likelihood ratio. 
Given that we only used the likelihoods in the decision, we used an 
uninformative prior. Once we decided whether the new parameter set 
would replace the old one, we ran the model again to sample the likelihood 
distribution of the parameter set. We then started 
the cycle again by proposing a new set of parameters and repeated
the process for $1e^5$ steps. We ran 5 independent chains, 
discarded the first $1000$ samples of each chain as burn in, 
and after that, we kept 1 in every 100 samples to avoid autocorrelation. 
The collection of parameters sets kept in 
all chains approximates the posterior distribution. We coded the model 
as well as the fitting algorithm in *c++*, the diagnostics and visualization 
in R [@rcoreteam_Language_2021]. All codes are accessible at 
\url{https://zenodo.org/badge/latestdoi/440585701}
<!-- [![zenodo.6338073.svg)](https://zenodo.org/badge/latestdoi/440585701). -->



To compare the fit of the three alternative models, we used the 
distribution of pseudo $R^2$ proposed by Mcfadden [@mcfadden_Conditional_1974].
Mcfadden's pseudo $R^2$ is a standard measure of fit for logistic regression. 
In that context, $pseudo-R^2$ uses the log-likelihood of the data given 
the model, relative to the log-likelihood of the data given a 
model without covariates, as a measure of fit. Our 
model is not a logistic regression, therefore we measured the $pseudo-R^2$ in
relation to the log-likelihood of a model
with parameters $\gamma$ and $\eta$ set to zero. This, in practice,
amounts to a model that has a neutral preference between the two options. 
We computed the pseudo $R^2$ for all the samples of the posterior from
the MCMC. We used these distributions of pseudo $R^2$'s as a measure of
fit. 


## Results

<!-- (ref:post) Posterior distributions for parameter values $\gamma$ and $\eta$ for  -->
<!-- the three models ("Full model", "chaining only" and "penalty only"). We show  -->
<!-- the kernel density estimates, below the mode (black dot) and the 65%  -->
<!-- (light blue shade) and 95% (grey shade)  highest posterior density interval  -->
<!-- for the two parameters. On the top, panels a and b show posterior  -->
<!-- distributions for a full model, including chaining ($\gamma$) and penalty  -->
<!-- ($\eta$). Panel c, shows the $\gamma$ estimate from model with only chaining.  -->
<!-- Panel d shows the $\eta$ estimate from a model with only penalty. Panel e,  -->
<!-- shows a measure of fit for all models, namely the distribution of pseudo-$R^2$  -->
<!-- obtained from sampling the posterior distribution of parameter values. -->


```{r post, fig.cap='Posterior distributions for parameter values $\\gamma$ and $\\eta$ for the six models, three for each type of agent (Agents: FAA and PAA; models: Full model, chaining only and penalty only). We show the kernel density estimates, below the mode (black dot) and the 66 (light color shade) and 95\\% (grey shade)  highest posterior density interval for the two parameters. Colors blue and red denote agents FAA and PAA respectively. On the top, panels a and b show posterior distributions for a full model, including chaining ($\\gamma$) and penalty ($\\eta$). Panel c, shows the $\\gamma$ estimate from model with only chaining. Panel d shows the $\\eta$ estimate from a model with only penalty. Panel e, shows a measure of fit for all models, namely the distribution of pseudo-$R^2$ obtained from sampling the posterior distribution of parameter values.',out.width='100%',fig.height=6,cache=FALSE}

scen1<-"MCMCclean_gam_Nrew_sca_"
scen2<-"MCMCclean_gam_sca_"
scen3<-"MCMCclean_Nrew_sca_"
scen4<-"MCMCclean_PAA2_gam_Nrew_sca_"
scen5<-"MCMCclean_PAA2_gam_sca_"
scen6<-"MCMCclean_PAA2_Nrew_sca_"

source(here("analysisMCMC.R"))
ggarrange(allGammapost.plot,alletapost.plot,common.legend = TRUE)
# first.col<-plot_grid(nrow=3,align = "v",byrow = TRUE,
#           gam.both.post.FAA,gam.gam.post.FAA,nrew.nrew.post.FAA,
#           labels=c('a','c','d'))
# second.col<-plot_grid(nrow = 2,align = "v",byrow = TRUE,
#           nrew.both.post.FAA,panel.rsqr.all,rel_heights = c(1,2),
#           labels = c('b','e'))
# plot_grid(ncol = 2,first.col,second.col)


```

```{r fitAllModels}
panel.rsqr.all
```


Estimation of parameter values for the three models (full model,
chaining and penalty), support chaining as the only mechanism 
cleaners use to account for the future effects of their actions;
and thus to solve the ephemeral reward task. In the estimation of the 
parameter values of the full model, which includes both chaining and
penalty, the bulk of the marginal posterior distribution of $\eta$ which 
controls the strength of penalty is around 0 (Fig. \ref{fig:post}). 
As for $\gamma$, controlling chaining, the 95% confidence 
intervals also includes zero, but the mode of the posterior 
is around 0.5 (Fig. \ref{fig:post}, a). In the chaining model, 
where $\eta$ is set to zero, the distribution of $\gamma$ shifted 
to higher values, zero is no longer part of the 95% credible interval
of the parameter (Fig. \ref{fig:post}, c). In contrast, when we 
look at the model with only penalty, the posterior distribution 
of $\eta$ is still centered around zero (Fig. \ref{fig:post}, d). 
Thus, the analysis of the estimates of individual parameter 
values in the three models only supports a strong 
effect of chaining. Furthermore, the comparison of the models' fit 
favors the chaining model. In panel e of figure \ref{fig:post} 
we show the distribution of $pseudo-R^2$ calculated using 
samples from the posterior distributions shown before. 
Note, $pseudo-R^2$ can have negative values, which is when
the log-likelihood of the model is lower than that of a model that
triggers neutral preferences. Even though the peak of the three
$pseudo-R^2$ distributions were not very different, the model with only
chaining produced a distribution of $pseudo-R^2$ where more values
were positive (to the right of the black line in Fig. \ref{fig:post} e). This
shows that accounting for variation in the parameter estimates the
model with chaining gives a better fit to the data, despite
having one parameter less than the full model. We have not shown 
here the marginal posterior distributions of the scaling constant, 
given that they do not bring biological insight. 
Their visualizations can be found in the supplementary 
material (Fig. \ref{fig:scaConstFAA} and \ref{fig:scaConstPAA}), as well as the diagnostics of the MCMCs
(Figs. \ref{fig:diagnosticsfullFAA},\ref{fig:diaggamFAA},\ref{fig:diagNegFAA},
\ref{fig:diagnosticsfullPAA},\ref{fig:diaggamPAA},\ref{fig:diagNegPAA}).



<!-- (ref:pred) Observed and predicted probability of choosing a visitor.  -->
<!-- Left-hand side panel: colour contour shows the prediction from the  -->
<!-- learning model using the mode of the posterior distributions of parameters  -->
<!-- recovered by the statistical analysis. Dots show the frequency of visitor  -->
<!-- choices for the 12 reef sites, as well as the corresponding relative  -->
<!-- cleaner fish abundance (x axis) and frequency of visitors clients leaving  -->
<!-- the cleaning station (y axis). Right-hand side panels: Variation of the  -->
<!-- predicted probabilities  of choosing a visitor over a resident and their  -->
<!-- observed values for 12 locations. Circles show the mean prediction for  -->
<!-- each location from 100 samples taken from the posterior distribution.  -->
<!-- Thick and thin bars show the 66 and 95% credible interval, respectively,  -->
<!-- taken from those posterior samples. Squares show the predictions used for the -->
<!-- panel on the right-hand side. Colour coding denotes different reef site/year  -->
<!-- of the data collection (see Empirical data section). The black line corresponds  -->
<!-- to a perfect match between observed and predicted probabilities. Upper  -->
<!-- panels (a and b) show predictions from a model including chaining and  -->
<!-- penalty; middle panels (c and d) from a  model with only chaining; lower  -->
<!-- panels (e and f) from a model with penalty only. -->

```{r predFAA, fig.cap='Observed and predicted probability of choosing a visitor. Left-hand side panel: colour contour shows the prediction from the learning model using the mode of the posterior distributions of parameters recovered by the statistical analysis. Dots show the frequency of visitor choices for the 12 reef sites, as well as the corresponding relative cleaner fish abundance (x axis) and frequency of visitors clients leaving the cleaning station (y axis). Right-hand side panels: Variation of the predicted probabilities  of choosing a visitor over a resident and their observed values for 12 locations. Circles show the mean prediction for each location from 100 samples taken from the posterior distribution. Thick and thin bars show the 66 and 95\\% credible interval, respectively, taken from those posterior samples. Squares show the predictions used for the panel on the right-hand side. Colour coding denotes different reef site/year of the data collection (see Empirical data section). The black line corresponds to a perfect match between observed and predicted probabilities. Upper panels (a and b) show predictions from a model including chaining and penalty; middle panels (c and d) from a  model with only chaining; lower panels (e and f) from a model with penalty only.',out.width='100%',fig.height=5,cache=FALSE}

scen1<-"MCMCclean_gam_Nrew_sca_"
scen2<-"MCMCclean_gam_sca_"
scen3<-"MCMCclean_Nrew_sca_"
 source(here("predictions.R"))
first_col<-plot_grid(nrow=2,align = "v",byrow = TRUE,
          cont.obs.pred.both,cont.obs.pred.gam,
          labels=c('a','c'),rel_heights = c(1.4,1))
second_col<-plot_grid(nrow=2,align = "v",byrow = TRUE,
          cont.obs.pred.Nrew,scatter.all.plot,
           rel_heights = c(1,1.4),
          labels=c('b','d'))
plot_grid(first_col,second_col)
```

```{r predPAA, fig.cap='Observed and predicted probability of choosing a visitor. Left-hand side panel: colour contour shows the prediction from the learning model using the mode of the posterior distributions of parameters recovered by the statistical analysis. Dots show the frequency of visitor choices for the 12 reef sites, as well as the corresponding relative cleaner fish abundance (x axis) and frequency of visitors clients leaving the cleaning station (y axis). Right-hand side panels: Variation of the predicted probabilities  of choosing a visitor over a resident and their observed values for 12 locations. Circles show the mean prediction for each location from 100 samples taken from the posterior distribution. Thick and thin bars show the 66 and 95\\% credible interval, respectively, taken from those posterior samples. Squares show the predictions used for the panel on the right-hand side. Colour coding denotes different reef site/year of the data collection (see Empirical data section). The black line corresponds to a perfect match between observed and predicted probabilities. Upper panels (a and b) show predictions from a model including chaining and penalty; middle panels (c and d) from a  model with only chaining; lower panels (e and f) from a model with penalty only.',out.width='100%',fig.height=5,cache=FALSE}

scen1<-"MCMCclean_PAA2_gam_Nrew_sca_"
scen2<-"MCMCclean_PAA2_gam_sca_"
scen3<-"MCMCclean_PAA2_Nrew_sca_"

source(here("predictions.R"))

first_col<-plot_grid(nrow=2,align = "v",byrow = TRUE,
          cont.obs.pred.both,cont.obs.pred.gam,
          labels=c('a','c'),rel_heights = c(1.4,1))
second_col<-plot_grid(nrow=2,align = "v",byrow = TRUE,
          cont.obs.pred.Nrew,scatter.all.plot,
           rel_heights = c(1,1.4),
          labels=c('b','d'))
plot_grid(first_col,second_col)
```


Low cleaner fish relative abundance yields different predictions 
from a model with only chaining and with penalty.
The model with only chaining predicts a low preference fpr the visitor when
cleaner fish relative abundance is low (Fig. \ref{fig:predPAA} c). In contrast,
the models including penalty (with and without chaining) predict high 
preference for the visitor option (Fig. \ref{fig:predFAA} a and e).
For intermediate and high cleaner relative abundances the predictions 
are similar for all three models. Intermediate relative abundance 
trigger a high preference for the visitor; while low abundances trigger
low preference for the visitor (Fig. \ref{fig:predFAA}a,b,e). 
Note, however, we calculated 
preferences shown in figure \ref{fig:predFAA} left panels by using only the model 
of the posterior distributions, and by holding constant the balance between 
resident and visitors' abundances. Panels on the right, show how close 
predictions are from the observed data, allowing the balance between 
client types to vary and using a set of samples from the posterior 
distribution. 

Visitor leaving probability has an overall 
positive effect on the probability of choosing the visitor
clients on all three models. All three models predict an increase preference 
for visitors as the visitor probability increases  (Fig. \ref{fig:predFAA}).


## Discussion

In this study, our main aim was to unravel which of two potential 
cognitive mechanisms, chaining of events, penalty, 
or their combination, best explains wild-caught cleaner fish 
performance in the ephemeral reward task, while accounting for their
ecological conditions. To evaluate the merits of each of these two mechanisms
separately and combined, we considered cleaner fish performance 
in the lab test to have its origin from the rule these fish applied 
in their natural environment. That is, individuals that 
solved the task already had a preference for 
visitor clients and generalized this rule to the lab conditions 
once being familiar with the task.

While all three models captured well the positive relationship between visitor 
leaving behavior and cleaner fish performance in
the market task [@triki_Biological_2019], only the chaining mechanism 
predicted that cleaner fish performance in the task should be low in 
habitats with low cleaner-to-client ratios, regardless of the 
visitor leaving probability. In contrast, models including 
negative reward predicted the highest performance in the ephemeral 
reward task when relative cleaner fish abundance is low, particularly 
together with a high probability of visitor leaving (Fig. \ref{fig:predFAA}).
Low relative cleaner fish abundances mean the market has an excess of demand for
cleaning services. In the models, this translates to a cleaning station
that is frequently full. Thus, when a visitor leaves, it is likely that the
cleaner fish will have access to another client in the next step. Therefore, 
there will not be much difference in future reward between choosing 
a visitor and a resident, and cleaners will not develop a preference for the
visitor in these conditions. On the other hand, the effect of negative reward 
on cleaner fish preference is the opposite, as in a busy cleaning station, to
that of chaining. Cleaner fish will get more often the resident-visitor
state and will develop a preference for the visitor faster. 
At high cleaner fish abundances, the resident-visitor state
becomes so rare that neither mechanism is very efficient 
at generating a preference for visitors. When facing the 
resident-visitor choice, it is still best to choose the visitor; however,
the learning machinery will not be able to develop this preference
efficiently. Overall, the models suggest that chaining is the cognitive
mechanism that allows cleaner fish to adaptively adjust to their 
biological market ecological conditions.

Previous research showed that cleaner fish living at high population 
densities and giving service priority to the visitor plate in the 
ephemeral reward task, as well as cleaner fish living at low 
densities but denying service priority to the visitor plate possess 
larger forebrains; a key teleost brain region associated with 
behavioural flexibility and social intelligence. Those failing to 
show optimized decision-rules given their local ecological 
conditions had relatively smaller forebrains  [@triki_Brain_2020]. 
Triki et al.  refer to the former as socially competent cleaner fish,
while the second group as socially incompetent cleaner fish. 
Social competence is the ability to optimise social behavior 
to the available social information [@taborsky_Social_2012a;
@bshary_Cooperation_2015;@varela_Correlated_2020]. Our analyses yielded no 
evidence that the difference in social competence with respect to the local 
ecological conditions and associated brain morphology, 
found by @triki_Brain_2020, is due to the mechanism 
used to incorporate future consequences. It is conceivable that 
high performing individuals from low population densities reef sites 
use negative reinforcement instead of chaining, but in that 
case, negative reinforcement should have explained at least 
part of the data. Configurational learning or 
chunking [@sutherland_Configural_1989;@miller_Magical_1956], 
the second component necessary to solve the ephemeral 
reward task [@quinones_Reinforcement_2019], was 
not varied in the models we analysed here. However, while chunking tendencies 
should vary to allow individuals to adapt to their local conditions 
[@prat_Modelling_2022;@kolodny_Evolution_2014], systematic differences 
in individual chunking tendencies would not explain how socially 
competent decisions vary as a function of relative abundance. 
Therefore, it remains currently unclear what cell-demanding 
mechanisms may cause variation in social competence that 
translates into site-specific variation in performance in the 
ephemeral reward task. 

Our models are inspired by the general processes of associative learning
where short term rewards are translated into decision making; thus, it 
ignores alternative channels of information that could be relevant in 
market-like situations. For example, the model does not investigate whether 
cleaner fish actually assess the frequency of client visits or a mean frequency 
of visitors leaving. The updating learning mechanism for the development 
of preferences works on a trial-by-trial basis. In the model, cleaner fish 
do not need to assess the actual state of the market, *i.e.* their abundance, 
the abundance of residents and visitors, and client visitation rate as an 
indicator of demand. They only need to assess the short-term 
consequences of their own decisions on food intake and chain them. 
Also, for the sake of simplicity, the model ignores 
the process by which cleaner fish discriminate 
residents and visitor clients. A model that accounts for this discrimination
probably would involve the development of preferences for morphological or 
behavioural features that are statistically associated with visitors 
or residents. For example, visitors are on average larger 
than residents in body size [@bshary_Cleaner_2001a], and contrary to 
residents, they are less likely to chase a cleaner fish that fails to cooperate
and instead cheats its client by taking a bite of mucus 
[@bshary_Asymmetric_2002]. Given these associations, chaining might 
produce the decision-rule “choose the larger client and/or the less aggressive 
client”, which is not a useful rule in the standard ephemeral reward task. 


In conclusion, our study shows that variation in cognitive performance as 
a function of the local ecological conditions may set the stage for the use of 
mechanistic modelling to identify the cognitive processes underlying 
learning in animals. The combination overcomes the limitations 
of the general philosophy in animal cognition to apply the logic of 
Lloyd Morgen's canon (Occam's razor). Cognitive experiments with the 
aim of excluding basic reinforcement learning as a potential 
explanation (operant and/or classical conditioning) of performance 
often employ one trial experiments requiring animals to solve the 
task on the first possible occasion. 
For example, any theory of mind task needs to be solved in the first
trial in order to exclude fast conditioning [@heyes_Theory_1998].
Similarly, subjects need to solve a social learning task on the first
trial to accept imitation as a mechanism over stimulus/local
enhancement. Such strict conditions are virtually never met. For
example, potato washing by Japanese macaques, an iconic example of
social learning, took several years to spread within the group
[@kawamura_Process_1959], meaning that any learner had been repeatedly
exposed to demonstrations before acquisition. 
Importantly, @galef_Question_1992 refuted imitation as a mechanism not simply because
of the repeated exposure but because a (rather qualitative) analysis of
the spread of potato washing across individuals did not follow the
prediction based on imitation learning (see also
@hirata_SweetPotato_2001). In our case, the number of trials it took
cleaners to learn the solution to the ephemeral rewards task 
would never allow excluding an important role of 
penalty based on the data
alone. However, fitting model predictions to our comprehensive empirical 
data set revealed that a more complex mechanism, estimation of future reward,
fits the data better.



\newpage

# Supplementary material


```{=tex}
\beginsupplement
```

<!-- (ref:rawdata)  -->


```{r rawdata,fig.cap='Relation between the response variable used in this study and the criteria used in previous studies to assess performance in the ephemeral reward task. In the x axis, we classified the performance of cleaner fish according to whether they developed a preference for the visitor in the initial round (initial only), in the initial and reversal (both), or none of them (none). In the y axis, we add the choices of two experimental sessions: in panel \\textbf{a} we use one session from the initial round and one from the reversal round when possible (as described in the main text); in panel \\textbf{b} we use two sessions from the initial round for all fish.',out.width='100%',fig.height=5,cache=FALSE}
plot_grid(initANDRev,init,ncol = 2,labels = c("a","b"),label_x = 0.15)

```

<!-- (ref:scaConst) Posterior distributions for scaling constant for the three models -->
<!-- ("Full model", chaining only" and "penalty only"). We show the kernel -->
<!-- density estimates, below the mode (black dot) and the 65% (light blue shade) -->
<!-- and 95% (grey shade)  highest posterior density interval. On the top, -->
<!-- panel a shows the posterior distribution from the full model; panel b from -->
<!-- the model with only chaining; and panel c from a model with only penalty. -->

```{r scaConstFAA, fig.cap='Posterior distributions for scaling constant for the three models (Full model, chaining only and penalty only). We show the kernel density estimates, below the mode (black dot) and the 65\\% (light blue shade) and 95\\% (grey shade)  highest posterior density interval. On the top, panel a shows the posterior distribution from the full model; panel b from the model with only chaining; and panel c from a model with only penalty.',fig.pos = 'H', out.extra = "",out.width='95%',fig.height=5}

sca.both.post<-ggplot(data=MCMCdata1,aes(x=scaleConst)) + 
  stat_halfeye(aes(fill=stat(cut(x,breaks = cuts.1$scaleConst))),
               point_interval = mode_hdi, .width = c(.66, .95),
               point_size=3,show.legend=FALSE) + 
  labs(title=labelsPlot.Scen[1],x="",y="")+
  theme_classic()+
  scale_fill_manual(values=c("gray85","skyblue","gray85"))
sca.gam.post<-ggplot(data=MCMCdata2,aes(x=scaleConst)) + 
  stat_halfeye(aes(fill=stat(cut(x,breaks = cuts.2$scaleConst))),
               point_interval = mode_hdi, 
               .width = c(.66, .95),point_size=3,
               show.legend=FALSE)+theme_classic()+
  scale_fill_manual(values=c("gray85","skyblue","gray85"))+
  labs(title=labelsPlot.Scen[2],x="",y="")
sca.nRew.post<-ggplot(data=MCMCdata3,aes(x=scaleConst)) +
  stat_halfeye(aes(fill=stat(cut(x,breaks = cuts.3$scaleConst[2:4]))),
               point_interval = mode_hdi, .width = c(.66, .95),
               point_size=3,
               show.legend=FALSE) +
  scale_fill_manual(values=c("skyblue","gray85"))+
  labs(title=labelsPlot.Scen[3],x="",y="")+
  theme_classic()
ggarrange(sca.both.post,sca.gam.post,
          sca.nRew.post,
          labels=c('a','b','c'),common.legend = F)

```

```{r scaConstPAA, fig.cap='Posterior distributions for scaling constant for the three models (Full model, chaining only and penalty only). We show the kernel density estimates, below the mode (black dot) and the 65\\% (light blue shade) and 95\\% (grey shade)  highest posterior density interval. On the top, panel a shows the posterior distribution from the full model; panel b from the model with only chaining; and panel c from a model with only penalty.',fig.pos = 'H', out.extra = "",out.width='95%',fig.height=5}

sca.both.post.PAA<-ggplot(data=MCMCdata4,aes(x=scaleConst)) + 
  stat_halfeye(aes(fill=stat(level)),
               point_interval = mode_hdi, .width = c(.66, .95),
               point_size=3,show.legend=FALSE) + 
  labs(title=labelsPlot.Scen[1],x="",y="")+
  theme_classic()+
  scale_fill_manual(values=c("gray85","skyblue","gray85"))
sca.gam.post.PAA<-ggplot(data=MCMCdata5,aes(x=scaleConst)) + 
  stat_halfeye(aes(fill=stat(level)),
               point_interval = mode_hdi, 
               .width = c(.66, .95),point_size=3,
               show.legend=FALSE)+theme_classic()+
  scale_fill_manual(values=c("gray85","skyblue","gray85"))+
  labs(title=labelsPlot.Scen[2],x="",y="")
sca.nRew.post.PAA<-ggplot(data=MCMCdata6,aes(x=scaleConst)) +
  stat_halfeye(aes(fill=stat(level)),
               point_interval = mode_hdi, .width = c(.66, .95),
               point_size=3,
               show.legend=FALSE) +
  scale_fill_manual(values=c("gray85","skyblue","gray85"))+
  labs(title=labelsPlot.Scen[3],x="",y="")+
  theme_classic()
ggarrange(sca.both.post.PAA,sca.gam.post.PAA,
          sca.nRew.post.PAA,
          labels=c('a','b','c'),common.legend = F)

```



<!-- (ref:diagnosticsfull) MCMC convergence diagnostics for the full model.  -->
<!-- On the left trace-plots, on the right changes along the chain of the  -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998]. -->


```{r diagnosticsfullFAA, fig.cap='MCMC convergence diagnostics for the full model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor.',out.width='100%',fig.height=5,cache=FALSE,message=FALSE}

longMCMCdata1<-melt(MCMCdata1,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","negReward","scaleConst"))
                    
parlab<-list(`gamma` =expression(gamma),
          `negReward`=expression(eta),
          `scaleConst`="Scaling constant")

traces.full<-ggplot(longMCMCdata1,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = function(variable,value){
  return(parlab[value])})+
  theme_classic()+
  theme(legend.position = 'top')

gel.full<-as.data.table(gelman.diag(mcmcList.1)$psrf)
gel.full[,parameter:=c("gamma","negReward","scalConst")]

pdf(file = NULL)
gp.dat<-gelman.plot(mcmcList.1)
rubbish <- dev.off()

gp.dat.frame <- data.table(rbind(as.data.frame(gp.dat[["shrink"]][,,1]), 
                          as.data.frame(gp.dat[["shrink"]][,,2])), 
                q=rep(dimnames(gp.dat[["shrink"]])[[3]], each=nrow(gp.dat[["shrink"]][,,1])),
                last.iter=rep(gp.dat[["last.iter"]], length(gp.dat)))

gelman.full<-ggplot(melt(gp.dat.frame, c("q","last.iter"), value.name="shrink_factor"), 
       aes(last.iter, shrink_factor, colour=q, linetype=q)) + 
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y  = element_blank())
  
ggarrange(traces.full,gelman.full)

```

<!-- (ref:diagnosticsGam) MCMC convergence diagnostics for the chaining model.  -->
<!-- On the left trace-plots, on the right changes along the chain of the  -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998] -->

```{r diaggamFAA,fig.cap='MCMC convergence diagnostics for the chaining model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor ',echo=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata2<-melt(MCMCdata2,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","scaleConst"))

longMCMCdata2[,parameter:=factor(parameter,labels=c("gamma","'Scaling constant'"))]
                    
traces.gam<-ggplot(longMCMCdata2,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = label_parsed)+
  theme_classic()+
  theme(legend.position = 'top')

gel.gam<-as.data.table(gelman.diag(mcmcList.2)$psrf)
gel.gam[,parameter:=c("gamma","scalConst")]

pdf(file = NULL)
gp.dat.gam<-gelman.plot(mcmcList.2)
rubbish <- dev.off()

gp.dat.frame.gam <- data.table(rbind(as.data.frame(gp.dat.gam[["shrink"]][,,1]), 
                          as.data.frame(gp.dat.gam[["shrink"]][,,2])), 
                q=rep(dimnames(gp.dat.gam[["shrink"]])[[3]], each=nrow(gp.dat.gam[["shrink"]][,,1])),
                last.iter=rep(gp.dat.gam[["last.iter"]], length(gp.dat.gam)))

gelman.gam<-ggplot(melt(gp.dat.frame.gam, c("q","last.iter"), value.name="shrink_factor"), 
       aes(last.iter, shrink_factor, colour=q, linetype=q)) + 
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.gam,gelman.gam)
```

<!-- (ref:diagNeg) MCMC convergence diagnostics for the penalty model. -->
<!-- On the left trace-plots, on the right changes along the chain of the -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998] -->

```{r diagNegFAA,fig.cap='MCMC convergence diagnostics for the penalty model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor',out.width='100%',fig.height=5,cache=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata3<-melt(MCMCdata3,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("negReward","scaleConst"))

longMCMCdata3[,parameter:=factor(parameter,labels=c("eta","'Scaling constant'"))]

traces.neg<-ggplot(longMCMCdata3,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = label_parsed)+
  theme_classic()+
  theme(legend.position = 'top')


pdf(file = NULL)
gp.dat.neg<-gelman.plot(mcmcList.3)
rubbish <- dev.off()

gp.dat.frame.neg <- data.table(rbind(as.data.frame(gp.dat.neg[["shrink"]][,,1]),
                          as.data.frame(gp.dat.neg[["shrink"]][,,2])),
                q=rep(dimnames(gp.dat.neg[["shrink"]])[[3]], each=nrow(gp.dat.neg[["shrink"]][,,1])),
                last.iter=rep(gp.dat.neg[["last.iter"]], length(gp.dat.neg)))

gelman.neg<-ggplot(melt(gp.dat.frame.neg, c("q","last.iter"), value.name="shrink_factor"),
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.neg,gelman.neg)
```


<!-- (ref:diagnosticsfull) MCMC convergence diagnostics for the full model. -->
<!-- On the left trace-plots, on the right changes along the chain of the -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998]. -->


```{r diagnosticsfullPAA, fig.cap='MCMC convergence diagnostics for the full model. On the left trace-plots, fig.height=5, message=FALSE, cache=FALSE, on the right changes along the chain of the Gelman and Rubin shrink factor.',out.width='100%'}

longMCMCdata4<-melt(MCMCdata4,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","negReward","scaleConst"))

parlab<-list(`gamma` =expression(gamma),
          `negReward`=expression(eta),
          `scaleConst`="Scaling constant")

traces.full.PAA<-ggplot(longMCMCdata4,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = function(variable,value){
  return(parlab[value])})+
  theme_classic()+
  theme(legend.position = 'top')

gel.full<-as.data.table(gelman.diag(mcmcList.4)$psrf)
gel.full[,parameter:=c("gamma","negReward","scalConst")]

pdf(file = NULL)
gp.dat<-gelman.plot(mcmcList.4)
rubbish <- dev.off()

gp.dat.frame.PAA <- data.table(rbind(as.data.frame(gp.dat[["shrink"]][,,1]),
                          as.data.frame(gp.dat[["shrink"]][,,2])),
                q=rep(dimnames(gp.dat[["shrink"]])[[3]], each=nrow(gp.dat[["shrink"]][,,1])),
                last.iter=rep(gp.dat[["last.iter"]], length(gp.dat)))

gelman.full.PAA<-ggplot(melt(gp.dat.frame.PAA, c("q","last.iter"), value.name="shrink_factor"),
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y  = element_blank())

ggarrange(traces.full.PAA,gelman.full.PAA)

```

<!-- (ref:diagnosticsGam) MCMC convergence diagnostics for the chaining model.  -->
<!-- On the left trace-plots, on the right changes along the chain of the  -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998] -->

```{r diaggamPAA,fig.cap='MCMC convergence diagnostics for the chaining model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor ',echo=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata5<-melt(MCMCdata5,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","scaleConst"))

longMCMCdata5[,parameter:=factor(parameter,labels = c("gamma","'Scaling constant'"))]


traces.gam.PAA<-ggplot(longMCMCdata5,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = label_parsed)+
  theme_classic()+
  theme(legend.position = 'top')

gel.gam<-as.data.table(gelman.diag(mcmcList.5)$psrf)
gel.gam[,parameter:=c("gamma","scalConst")]

pdf(file = NULL)
gp.dat.gam<-gelman.plot(mcmcList.5)
rubbish <- dev.off()

gp.dat.frame.gam.PAA <- data.table(rbind(as.data.frame(gp.dat.gam[["shrink"]][,,1]),
                          as.data.frame(gp.dat.gam[["shrink"]][,,2])),
                q=rep(dimnames(gp.dat.gam[["shrink"]])[[3]], each=nrow(gp.dat.gam[["shrink"]][,,1])),
                last.iter=rep(gp.dat.gam[["last.iter"]], length(gp.dat.gam)))

gelman.gam.PAA<-ggplot(melt(gp.dat.frame.gam.PAA, c("q","last.iter"), value.name="shrink_factor"),
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.gam.PAA,gelman.gam.PAA)
```

<!-- (ref:diagNeg) MCMC convergence diagnostics for the penalty model. -->
<!-- On the left trace-plots, on the right changes along the chain of the -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998] -->

```{r diagNegPAA,fig.cap='MCMC convergence diagnostics for the penalty model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor',out.width='100%',fig.height=5,cache=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata6<-melt(MCMCdata6,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("negReward","scaleConst"))

longMCMCdata6[,parameter:=factor(parameter,labels = c("eta","'Scaling constant'"))]

traces.neg.PAA<-ggplot(longMCMCdata6,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = label_parsed)+
  theme_classic()+
  theme(legend.position = 'top')


pdf(file = NULL)
gp.dat.neg<-gelman.plot(mcmcList.6)
rubbish <- dev.off()

gp.dat.frame.neg.PAA <- data.table(rbind(as.data.frame(gp.dat.neg[["shrink"]][,,1]),
                          as.data.frame(gp.dat.neg[["shrink"]][,,2])),
                q=rep(dimnames(gp.dat.neg[["shrink"]])[[3]], each=nrow(gp.dat.neg[["shrink"]][,,1])),
                last.iter=rep(gp.dat.neg[["last.iter"]], length(gp.dat.neg)))

gelman.neg.PAA<-ggplot(melt(gp.dat.frame.neg.PAA, c("q","last.iter"), value.name="shrink_factor"),
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.neg.PAA,gelman.neg.PAA)
```
```{r diagbothGroup,fig.cap='MCMC convergence diagnostics for the penalty model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor',out.width='100%',fig.height=5,cache=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata7<-melt(MCMCdata7,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","probFAA","probFAA.1","scaleConst"))

longMCMCdata7[,parameter:=factor(parameter,labels = c("gamma","p[FAA]","p[FAA_1]","'Scaling constant'"))]

traces.both.group<-ggplot(longMCMCdata7,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = label_parsed)+
  theme_classic()+
  theme(legend.position = 'top')


pdf(file = NULL)
gp.dat.both.group<-gelman.plot(mcmcList.7)
rubbish <- dev.off()

gp.dat.frame.both.group <- data.table(rbind(as.data.frame(gp.dat.both.group[["shrink"]][,,1]),
                          as.data.frame(gp.dat.both.group[["shrink"]][,,2])),
                q=rep(dimnames(gp.dat.both.group[["shrink"]])[[3]], each=nrow(gp.dat.neg[["shrink"]][,,1])),
                last.iter=rep(gp.dat.both.group[["last.iter"]], length(gp.dat.both.group)))

gelman.both.group<-ggplot(melt(gp.dat.frame.both.group, c("q","last.iter"), value.name="shrink_factor"),
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.both.group,gelman.both.group)
```

```{r param , echo=FALSE,results='asis'}
  cat(' Table: \\label{tab:param} Parameter values with which the model was run 
  in the MCMC. $\\sigma$ refers to the amplitude of the perturbation kernel with the subscript indicating the associated parameter. New values were taken from a uniform distribution. $\\alpha$ refers to the learning rate. 

  | Parameter | Value |
|----------:|:-----:|
| Learning rounds | 10000 |
| Reward value | 1 |
| $\\alpha$ | 0.05 |
| $\\sigma_{\\gamma}$ | 0.3 |
| $\\sigma_{\\eta}$ | 4 |
| $\\sigma_{Sca.Const.}$ | 300 |
| Number of chains | 5 |
| Chain length | $1^5$ |
')

```


# References {#references .unnumbered}