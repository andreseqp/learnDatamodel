---
title: Field and experimental data together with computational 
  models reveal how cleaner fish adjust decisions  in a biological market
author:
  - name: Andrés E. Quiñones 
    email: andresqp@gmail.com
    affiliation: a
    corresponding: andresqp@gmail.com
  - name: Zegni Triki
    email: zegni.triki@gmail.com
    affiliation: b
  - name: Redouan Bshary
    email: redouan.bshary@unine.ch
    affiliation: Institute of Biology, University of Neuchatel
address:
  - code: a
    address: Institute of Biology, University of Neuchatel, Rue Emile-Argand 11, 2000-CH Neuchatel, Switzerland
  - code: b
    address: Department of Zoology, Stockholm University, Svante Arrheniusväg 18 B,  106 91 Stockholm, Sweden,
abstract: |
  While it is generally straightforward to quantify individual performance in cognitive experiments, identifying the underlying cognitive processes remains a major challenge. Often, different mechanistic underpinnings yield similar performances, and Lloyd Morgan’s cannon warrants acceptance of the simpler explanation. Alternatively, when the different mechanisms interact with environmental conditions, variation in performance across environments might allow to statistically infer the mechanism responsible. We illustrate this point by fitting computational models to experimental data on performance by wild-caught cleaner fish *Labroides dimidiatus* in an ephemeral reward task, as well as cleaner and client fish densities from the locations of capture. Using Bayesian statistics to fit the model parameters to performance data revealed that cleaner fish most likely estimate future consequences of an action, while it appears unlikely that the removal of the ephemeral reward acts as psychological punishment (negative reinforcement). Incorporating future consequences also yields performances that can be considered the result of locally optimal decision-rules, in contrast to the negative reinforcement mechanism. We argue that the combination of computational models with data is a powerful tool to infer the mechanistic underpinnings of cognitive performance. 
  
author_summary: |
  Performance in behavioural experiments is often used to assess the cognitive abilities of animals. However, animals can get to the same outcome in alternative ways. Thus, the outcome of the experiments does not tell us how animals achieve their performance.  In order to overcome this limitation, we used a set of computational models, which provide predictions on how alternative cognitive mechanisms perform in the face of varying environmental conditions, and compare their predictions to performance data from individuals that experienced different conditions throughout their life. Our study system is the cleaner fish *Labroides dimidiatus*, a coral reef fish that feeds off ectoparasites and dead tissue from the skin of other reef fishes. Cleaner fish often have to choose among alternative clients seeking their cleaning service, and the optimal choice is often hard to achieve. Our performance experiments mimic the natural choices the cleaner fish make. The combination of experiments and computational models points to cleaner fish abilities to account for the future effect of their choices by estimating the long term expected value of those choices. Estimating long term value is a mechanism involved in human foresight. 

bibliography: Cleanerlearning.bib
header-includes: |
  ```{=latex}
  \usepackage{float}
  \usepackage{booktabs}
  \newcommand{\beginsupplement}{ \setcounter{table}{0}     \renewcommand{\thetable}{S\arabic{table}}\setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
  ```
output: rticles::plos_article
# plos.csl can be download and used locally
csl: http://www.zotero.org/styles/plos
---

_Text based on plos sample manuscript, see <https://journals.plos.org/ploscompbiol/s/latex>_

## Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning = FALSE,message=FALSE,cache = TRUE)
library(here)
source(file=here("loadFieldData.R"))
```

Often alternative cognitive mechanisms yield similar behaviour and/or cognitive
performances. This poses a problem for disentangling the mechanistic 
underpinnings of behaviour. This is particularly clear in research aimed at
discovering between species variation in *higher* cognitive abilities; or 
in other words, research on whether non-human animals show cognitive 
abilities believed to be uniquely human. For instance, when researchers 
try to find *mental time travel-like* behaviour, they usually come up with 
clever experiments to show the behaviour displayed requires inferences made 
through past events[@dally_Foodcaching_2006]. However, they often face the
challenge of alternative scenarios where simpler explanations, 
like classic associative learning, can bring about the 
observed behavioural outcome [@suddendorf_Evolution_2007]. Similarly, attempts 
to demonstrate the presence of *theory of mind* in non-human animals 
face objections justified by alternative mechanisms underpinning 
similar behavioural results [@heyes_Theory_1998]. 
Such controversies are usually settled by using the principle 
of parsimony and its cognitive version, 
Lloyd Morgan’s cannon, which states that the simpler explanation (mechanism)
should be accepted. Ideally, however, alternative hypotheses should be evaluated 
in light of their explanatory power. One potential dimension that could allow
the variation necessary among distinct cognitive mechanisms is environmental
variation. Controlled laboratory conditions often mask between and within 
species variation that arises among different environments. Thus, if 
predictions from the alternative mechanisms depend on environmental 
conditions, it is possible to assess their relative explanatory power 
by performing cognitive test in the context of
different environmental conditions. 

From an evolutionary perspective, mechanisms are likely selected because 
of how they allow individuals to respond to environmental variation. 
For example, biological market theory predicts that the exchange rate of goods 
and/or services traded between cooperative partners adjusts to the 
law of supply and demand, when individuals have some degree of partner choice
[@noe_Biological_1995]. Supply and demand conditions, which typically 
depend on the abundance of the species involved, certainly vary in time 
and space. Therefore, natural selection should favour the ability 
to flexibly adjust decisions and behavioural output to 
current market conditions. Indeed, such adjustments have been repeatedly 
documented [@axen_Signalling_1996]. In animals, an obvious general candidate 
mechanism for the strategic adjustment is the cognitive machinery. However, 
it is not clear which cognitive mechanisms allow individuals to adjust their 
behaviour to the varying conditions. More specifically the question arises 
to what extent mechanisms beyond basic associative learning may be involved.

One example of strategic adjustment in a biological market is the marine
cleaning mutualism involving the cleaner fish *Labroides dimidiatus* and
'client' fish. Client fish seek cleaner fish services at their territory 
(so-called “cleaning station”) and offer themselves as food patches 
to get their ectoparasites removed, which provides cleaners 
with food and clients with improved health [@waldie_LongTerm_2011;
@ros_Does_2011;@triki_Effects_2016;@demaire_Reduced_2020]. 
Given the capacity of some client fish to swim larger distances and 
access multiple cleaning stations while others access the only cleaning 
station in their territory, it is crucial to categorize clients as 
either “visitors” or “residents”, respectively. During cleaning interactions, 
a cleaner fish often face a choice between a visitor and a resident client 
seeking its cleaning services simultaneously. Visitors have the option 
to switch to another cleaner fish if being made to wait, while residents 
must wait for inspection. Indeed,
visitors have been observed to use their partner choice option in that
way [@bshary_Choosy_2002], which may explain why cleaners give
visitors service priority in a field study in the Red Sea
[@bshary_Cleaner_2001]. 

When researchers aimed at testing wild-caught cleaner fish, as 
well as individuals from other species, abilities 
to prioritize visitor over resident clients in a lab-based paradigm, 
they used Plexiglas plates of different colours and shapes offering the same 
amount of food as surrogates for visitor and resident clients.
One plate is made to behave like a resident, i.e. it remains until 
the cleaner fish has eaten off it. The other plate was made to behave 
like a visitor, i.e. it is removed if not inspected first. 
This laboratory paradigm originally inspired by the cleaning market 
of cleaner fish is more generally referred as the ephemeral reward task. 
Individuals are assessed in their capacity to develop a preference for the 
ephemeral option, which gives them access to two reward units.
Cleaners learned to prefer the visitor plate and
hence obtained the double amount of food [@bshary_Asymmetric_2002].
Furthermore, adult cleaner fish outperformed various primates as well as rats and
pigeons in this original version of the ephemeral reward task
[@salwiczek_Adult_2012; @zentall_Early_2017]; yet the African grey
parrots solve this task as well [@pepperberg_Can_2014]. 
This between variation in performance, and primates' known cognitive abilities
[@reader_Evolution_2001], shows that performance in the task
cannot be readily explained by classifying species as having high and 
low cognitive abilities. Instead, this between species variation suggests
that solving the ephemeral reward task requires a set of cognitive 
mechanisms that are not present in all the tested taxa, or were not used 
under the experimental set-up. 

Recent research on the cognitive abilities needed to solve the ephemeral 
reward task has used a broad approach as proposed by Shettleworth 
[@shettleworth_Cognition_2009], who defined cognition as including 
all ways in which animals acquire information 
through the senses, process, retain and decide to act on it. 
Indeed, perception of relevant cues is of major importance 
for individual performance. For example, some animals find relevant 
information in the plates  [@wismer_Cuebased_2019] while others find 
it in the food [@pretot_Comparative_2021; @pretot_Comparing_2016]. 
However, identifying cues as salient is not the only challenge 
of the ephemeral reward task, as revealed by proximate learning models. 
Such models allow varying the cognitive tool kit and evaluating 
which minimal kit is necessary to solve 
the task at hand (e.g. [@dubois_Model_2021]). Applied to the ephemeral 
reward task, learning models showed that basic reinforcement learning does 
not suffice to solve the ephemeral reward task [@prat_Modelling_2022;
@quinones_Reinforcement_2019]. 
This is particularly so when models assume the more complex natural 
situation in which cleaner fish face not only resident-visitor pairs but also 
visitor-visitor and resident-resident pairs, as well a resident alone or 
a visitor alone. To be able to give visitors priority over residents, 
cleaners need to be able to assess a client’s value separately for the 
three possible scenarios (alone, paired with a fish with the 
same strategic option, paired with a fish with the alternative strategic option) 
[@quinones_Reinforcement_2019]. The ability to distinguish and value 
one stimulus differently alone from compound versions of it has been termed 
configurational learning, chunking, 
or segmentation (see references in [@prat_Modelling_2022]). 

In addition to configurational learning, cleaners also need to account for 
the future consequences of current decisions. In the model by 
Quiñones *et al.* [@quinones_Reinforcement_2019], this could be achieved in 
two non-mutually exclusive ways: through low temporal discounting of 
future effects, also termed ‘chaining’ [@enquist_Power_2016]; and/or 
through perceiving a visitor client leaving as psychological punishment 
(i.e. as a negative reinforcer). Low temporal discounting is when 
individuals include in their valuation of an action the reward effects 
that this will have in the future. This is done by combining in a single 
valuation the reward obtained in the current time with all the reward that 
comes after, discounting for how far in the future reward 
is accrued. 'Chaining' the reward of these different time steps allows 
individuals to take actions that increase the long-term reward at the sacrifice
of short term considerations [@enquist_Power_2016]. Even though, 'chaining' 
can be readily implemented computationally in learning models 
[@enquist_Power_2016;@sutton_Reinforcement_2018], cognitively it seems to be 
a complex adaptation [@suddendorf_Evolution_2007]. On the other hand,
using client behaviour as a negative reinforcer is, in principle, 
easier to implement. Negative reinforcement is part of the ubiquitous 
learning mechanism termed operant conditioning [@thorndike_Animal_1898; 
@skinner_Behavior_1938]. Thus, the standard logic of 
Lloyd Morgan’s cannon demands that operant conditioning as the simpler 
explanation is to be accepted by default. Ideally, however, the two 
mechanisms should be evaluated in light of how well 
they explain the available data. Note that different fields interested in
cognition and decision making use different words to refer to negative 
reinforcers [@quinones_Reinforcement_2019;@sutton_Reinforcement_2018]. 
Here, for the sake of simplicity and clarity, we will use the word 
'penalty' to refer to this mechanism which includes a negative reinforcer. 


Over the last decade, our team tested over a hundred wild-caught 
cleaner fish in the exact same paradigm of the ephemeral reward 
task [@salwiczek_Adult_2012;@wismer_Variation_2014;@triki_Decrease_2018;
@triki_Biological_2019;@triki_Brain_2020]. These fish often come from 
different reef locations around the study field site at Lizard Island, 
Great Barrier Reef in Australia. Further investigation of the local 
eco-sociological conditions revealed that cleaner and client 
fish population densities have a substantial impact on cleaner fish 
performance in the task. Cleaner fish from reef sites with relatively 
low densities were more likely to fail at solving the task [@triki_Biological_2019;
@triki_Decrease_2018; @wismer_Variation_2014]. The explanation for this pattern
is twofold. First, low cleaner fish population densities imply 
low supply and high demand for cleaning services. 
Under such conditions, there are fewer occasions under 
which failing to give priority to a visitor translates to an empty 
cleaning station. Second and also related to the state of the cleaning market,
visitor clients at low cleaner fish population densities 
are less likely to exert their partner choice options 
and are hence more likely to wait for inspection if not given cleaning 
priority [@triki_Brain_2020]. In the absence of partner choice 
cleaner fish should not give priority to visitors any more. 

The interplay between the underlying cognitive machinery and 
local ecological conditions apparently generate the documented variable 
performance among individuals of the same species. 
Our approach of fitting the computational model to the empirical data on 
fish densities and cleaner fish censuses and performance
in the ephemeral reward task aimed at: (i), determining which 
mechanism cleaner fish use to incorporate future consequences of current 
decisions by testing whether chaining, penalty, or a combination 
of both best explains their performance; (ii)  determining whether the 
two mechanisms differed with respect to the ecological conditions 
that are likely to cause high versus low performance in the 
ephemeral reward task. Additionally, we assessed which mechanism 
yields optimal performance patterns. Relying on the logic of 
biological market theory, we predicted 
that appropriate performance is to show a low preference for visitors under low
local cleaner-to-client ratio. 

## Methods


### The model

The model consists of a set of individual-based simulations where
individuals, representing cleaner fish, face a series of choices 
between two options, which simulate the natural conditions 
of the cleaning market. Individuals experience a series 
of discrete time points in which they face different ‘states’, 
defined by the number and category of client fish (visitor or residents) 
inviting for cleaning services. There are six possible states: 
zero clients, one resident, one visitor, 
resident-resident, visitor-visitor, and resident-visitor. 
The probability of each state is largely determined by the relative 
abundance of cleaner fish, residents and visitors, but to some degree by
cleaner fish choices when it faces the resident-visitor combination. 
This is because residents are willing to queue for cleaning service; 
while visitors leave the queue (with a certain probability) when made to wait. 
Individuals obtain a fixed reward from cleaning a client fish
regardless of the category. Every time individuals face and make a choice
they update the probability of making that same choice. The update is
based on the difference between the expected reward and the obtained
reward (prediction error) [@sutton_Reinforcement_2018; 
@rescorla_Theory_1972]. Given the new information, the update is carried
in the direction that leads to more reward being obtained, given
the new information. In the long run, the probability of choosing a visitor 
over a resident converges in the model. To which probability the model 
will converge depends on the relative abundance of cleaners, visitors 
and residents; as well as on the probability of visitors leaving the 
cleaning station when made to wait. Further details of the model 
implementation can be found in Quiñones *et al* [@quinones_Reinforcement_2019]. 

The model shows that agents need to find a way 
to incorporate future consequences of current choices. In the model, 
this could be achieved with either of two parameters that could 
also work together.  First, $\gamma$ measures how
much individuals include future rewards in their decision updates. 
If $\gamma=0$, individuals only use the immediate reward obtained from a
cleaning interaction. As $\gamma$ increases, individuals include more the
reward obtained from the subsequent choices. That amounts to calculating
and using for decision making the future expected rewards of an action (chaining).
Second, $\eta$ measures how much individuals include in their reward the
fleeing behaviour of visitors as a negative component (penalty). Both of
these parameters allow individuals to use in their estimates the future
effects of their choices. 


### Empirical data

The empirical data were collected between 2010 and 2019 always during 
the austral winter months June to August from a total of five 
study reef sites (Corner Beach-CB, Horseshoe-HS, Mermaid Cove-MC, 
Northern Horseshoe-NHS, and The Crest-TC)  at Lizard Island 
($14.6682^\circ S, 145.4604^\circ E$), Great Barrier Reef, Australia. 
The data consist of three sets: fish censuses, field 
observations of cleaner-client interactions to quantify the probability 
of visitors leaving if made to wait, and the 
performance of wild-caught cleaner fish in the ephemeral reward test. 
In total, we have twelve site/year data sets for fish censuses and 
corresponding performance in the lab test. Thus, some sites were sampled more 
than once. To estimate the population density of cleaner fish 
and their clients at a given site in 
a given year, Triki *et al.* [@triki_Biological_2019] used a series of ten 
transects of $30m$ each. Observers swam along the transect lines placed on 
the reef and first counted the visible large-bodied adult fish 
(species with total length $TL \geq 10cm$) including cleaner fish 
on a width of $5m$, and then on the 
return individuals of small-bodied fish species ($TL < 10 cm$) \
on a width of $1 m$
(see Triki et al. 2018 for further details on fish censuses data collection). 
We then scaled the counts of cleaner fish, small-bodied, 
and  large-bodied  clients fish densities per $100 m^2$. 
From the study by Triki *et al* [@triki_Biological_2019] 
cleaner fish and large-bodied client populations densities were 
highly correlated, and only the former 
was hence used in the analyses as representative of both measures.

The field observation data consisted of video recordings/encodings of the 
cleaner-client cleaning interactions. There were videos from eight 
cleaners per site/year of a duration of $30 min$ each. Triki *et al* 
extracted information from every event wherein a visitor client 
was made to wait in favour of another client (visitor or resident), and noted 
whether or not the visitor left or queued for the cleaning service
[@triki_Biological_2019;@triki_Brain_2020]. 

The cognitive performance data was from a total of 120 cleaners 
(10 individuals per 12 site/year) tested in the 
ephemeral reward task [@triki_Biological_2019;@triki_Brain_2020]. Authors 
housed all captured cleaners individually in glass aquaria 
( $62cm \times 27cm \times 37 cm$ ) and provided them 
with PVC pipes ($10 cm \times 1 cm$) as shelters. 
The task consisted of exposing the cleaner fish to substitute 
models of client fish in the form of two *Plexiglas* plates offering the 
same amount of food (one item of mashed prawn).  The two plates differed 
in colour and pattern (horizontal green stripes or vertical pink stripes) 
but had equal size ($10 cm \times 7 cm$). Importantly, the two plates played 
different roles as either a visitor (ephemeral food source) or 
resident (permanent food source). That is, if a cleaner fish inspected the 
resident plate first, the experimenter withdrew the visitor plate out of 
the aquarium as a consequence.  Choosing first the visitor plate, 
however, granted access to both plates. The equal size of the plates 
forced cleaner fish to learn to give service priority to the visitor plate 
based solely on the behaviour-cue of the plates rather than size-cue 
[@wismer_Cuebased_2019]. Triki *et al.* [@triki_Biological_2019;@triki_Brain_2020] 
tested the fish for a maximum of 200 trials with 20 trials a day, 10 trials 
in the morning and 10 trials in the afternoon. They randomized and 
counterbalanced the plates’ spatial location (i.e. left or right) 
between trials. Similarly, they counterbalanced the plates’ decoration 
(colour and pattern) and the plates’ role (visitor or resident) between the 
tested fish. In the original studies, once a fish reach a learning criterion, 
that is, performing significantly above chance level ($> 50%$, $p-value ≥ 0.05$),
they passed to a reversal version of the task where the roles of the 
visitor/resident Plexiglas plates were swapped (see Triki et al. 2019, 2020). 
Here, we used instead a subset of these data in order to have an idea of 
cleaner fish preferences for the visitor plate, even if they do not reach 
the learning criterion within 200 trials. To do so, we first extracted the 
trial-by-trial outcomes from the last two sessions (20 trials) of those who 
never reached the learning criterion for visitor plate 
($N = `r fieldData.cleaner.filt.sum[V1==2,length(V1)]`$ cleaner fish). 
For those who reached the learning criterion at some point during the 
test and passed to a reversal phase, we extracted the trial-by-trial 
outcomes from the last session (10 trials) before passing to reversal 
and the last 10 trials they were exposed to in the test 
($N = `r fieldData.cleaner.filt.sum[V1>2,length(V1)]`$ cleaner fish). 
We chose a combination of initial and reversal to quantify preference 
for the visitor client, given that it matches well the criteria chosen 
in previous analysis of the ephemeral reward task experimental set-up
[@triki_Biological_2019;@triki_Brain_2020]. In the 
supplementary material (Fig. \ref{fig:rawdata}) we show how our choice maps
to the previously used criteria. For comparison, we also show a quantification 
of visitor preference based only on the initial phase of the task. 



### Statistical analysis

The aim of the analyses is to fit the key model parameters 
$\gamma$ and $\eta$, to the empirical data from Triki *et al.* 
[@triki_Biological_2019;@triki_Brain_2020] to test whether each or 
a combination of these effects is a better explanation for the pattern 
seen in the data. We used the ecological variables: cleaners, visitor clients, 
resident clients abundances and visitor clients leaving probability,  
as input to the models. As the response variable, we used cleaners' 
preference for visitors over residents in the ephemeral reward task. 
Finally, we used the preference for visitors resulting from the 
model simulations as the prediction for the response variable. 
We kept all other parameter values used for the model simulations constant, 
see Table \ref{tab:param}.

To capture with the model the relationship between the ecological 
variables and cleaner fish preferences for visitors, we needed to scale 
the absolute population densities of cleaner fish from the empirical data
to a measure of relative abundances that captures client visitation patterns. 
This is because, in the model, relative abundances of clients 
define not only the probability of residents and visitors 
but also how often the cleaning station is empty (e.g. there are no 
clients to be cleaned). The frequency with which clients visit the station 
is another variable, which in nature may vary among different client species 
depending on their ectoparasite loads. We do not have field estimates 
for species-specific parasite loads, especially not as a function of the site. 
In order to control for these aspects, we computed a measure of relative 
cleaner fish abundance for each reef site relative to 
absolute abundances and multiplied it
by a scaling constant that changes the range of the variable. This scaling 
constant is hence meant to capture variation in the market conditions driven 
partly by cleaner fish abundance. We fitted the value of the scaling constant as 
part of the statistical inference. As for the visitor and resident 
abundances, we computed a relative measure with respect to the 
total client abundance, and weighted that by the rescaled relative absence 
of cleaners ($1-\text{relative cleaner abundance}$). Thus, all three measures 
of relative abundance sum up to 1, and can be used in the model as a proxy 
for the probability of having different options in the cleaning station. 
Note that we introduced the scaling constant to control for variation that 
is not captured by the model; its parameter distribution does not offer 
biological insights.

Once we calculated the relative abundances, we obtained predictions from 
the model for each one of the locations and ran the Markov Chain. We started the
chain with random values for the parameters of interest, then ran the 
computational model once for every reef site using as input the ecological 
explanatory variables. The model outputs the probability of choosing the visitor
for each location $p_i$, where $i$ is the index for the 12 locations. Assuming a 
binomial distribution, the probability that each of the cleaner fish 20 choices 
in the ephemeral reward task was generated by the model 
is given by $\binom{n}{k_ij}p^{k_{ij}}_i (1-p_i)^{n-k_{ij}}$;
where $k_{ij}$ is the number of times that cleaner fish $j$ in reef site $i$ 
chose the visitor over the resident; and $n=20$ (due to our choice of using 20 
choices per cleaner fish). By taking the natural logarithm and summing over all 
the cleaner fish and reef sites, we obtained the log-likelihood of the data 
given the model and parameters. We then proposed a new set of parameters drawn 
from a uniform distribution centred around the old parameter set. The amplitude 
of the uniform distribution used for each parameter can be found in Table 
\ref{tab:param}. Subsequently, we ran the model and calculated the likelihood 
with the new parameter set. We then used the ratio of the two likelihoods to 
choose which parameter set to keep. New parameter sets with a higher likelihood 
than the old set replaced old ones, and those with a lower likelihood replaced 
current ones  with a probability equal to the log-likelihood ratio. 
Given that we only used the likelihoods in the decision, we used an 
uninformative prior. Once we decided whether the new parameter set 
would replace the old one, we ran the model again to sample the likelihood 
distribution of the parameter set. We then started 
the cycle again by proposing a new set of parameters and repeated
the process for $1e^5$ steps. We ran 5 independent chains, 
discarded the first $1000$ samples of each chain as burn in, 
and after that, we kept 1 in every 100 samples to avoid autocorrelation. 
The collection of parameters sets kept in 
all chains approximates the posterior distribution. We coded the model 
as well as the fitting algorithm in *c++*, the diagnostics and visualization 
in R [@rcoreteam_Language_2021]. All codes are accessible at 
<https://github.com/andreseqp/learnDatamodel>

To compare the fit of the three alternative models, we used the 
distribution of pseudo $R^2$ proposed by Mcfadden [@mcfadden_Conditional_1974].
Mcfadden's pseudo $R^2$ is a standard measure of fit for logistic regression. 
In that context, $pseudo-R^2$ uses the log-likelihood of the data given 
the model, relative to the log-likelihood of the data given a 
model without covariates, as a measure of fit. Our 
model is not a logistic regression, therefore we measured the $pseudo-R^2$ in
relation to the log-likelihood of a model
with parameters $\gamma$ and $\eta$ set to zero. This, in practice,
amounts to a model that has a neutral preference between the two options. 
We computed the pseudo $R^2$ for all the samples of the posterior from
the MCMC. Thus, we used these distributions of pseudo $R^2$'s as a measure of
fit. 


## Results

<!-- (ref:post) Posterior distributions for parameter values $\gamma$ and $\eta$ for  -->
<!-- the three models ("Full model", "chaining only" and "penalty only"). We show  -->
<!-- the kernel density estimates, below the mode (black dot) and the 65%  -->
<!-- (light blue shade) and 95% (grey shade)  highest posterior density interval  -->
<!-- for the two parameters. On the top, panels a and b show posterior  -->
<!-- distributions for a full model, including chaining ($\gamma$) and penalty  -->
<!-- ($\eta$). Panel c, shows the $\gamma$ estimate from model with only chaining.  -->
<!-- Panel d shows the $\eta$ estimate from a model with only penalty. Panel e,  -->
<!-- shows a measure of fit for all models, namely the distribution of pseudo-$R^2$  -->
<!-- obtained from sampling the posterior distribution of parameter values. -->


```{r post, fig.cap='Posterior distributions for parameter values $\\gamma$ and $\\eta$ for the three models (Full model, chaining only and penalty only). We show the kernel density estimates, below the mode (black dot) and the 65 (light blue shade) and 95\\% (grey shade)  highest posterior density interval for the two parameters. On the top, panels a and b show posterior distributions for a full model, including chaining ($\\gamma$) and penalty ($\\eta$). Panel c, shows the $\\gamma$ estimate from model with only chaining. Panel d shows the $\\eta$ estimate from a model with only penalty. Panel e, shows a measure of fit for all models, namely the distribution of pseudo-$R^2$ obtained from sampling the posterior distribution of parameter values.',out.width='100%',fig.height=6,cache=FALSE}

source(here("analysisMCMC.R"))
first.col<-plot_grid(nrow=3,align = "v",byrow = TRUE,
          gam.both.post,gam.gam.post,nrew.nrew.post,
          labels=c('a','c','d'))
second.col<-plot_grid(nrow = 2,align = "v",byrow = TRUE,
          nrew.both.post,panel.rsqr.all,rel_heights = c(1,2),
          labels = c('b','e'))
plot_grid(ncol = 2,first.col,second.col)


```

Estimation of parameter values for the three models (full model,
chaining and penalty), support chaining as the only mechanism 
cleaners use to account for the future effects of their actions;
and thus to solve the ephemeral reward task. In the estimation of the 
parameter values of the full model, which includes both chaining and
penalty, the bulk of the marginal posterior distribution of $\eta$ which 
controls the strength of penalty is around 0 (Fig. \ref{fig:post}). 
As for $\gamma$, controlling chaining, the 95% confidence 
intervals also includes zero, but the mode of the posterior 
is around 0.5 (Fig. \ref{fig:post}, a). In the chaining model, 
where $\eta$ is set to zero, the distribution of $\gamma$ shifted 
to higher values, zero is no longer part of the 95% credible interval
of the parameter (Fig. \ref{fig:post}, c). In contrast, when we 
look at the model with only penalty, the posterior distribution 
of $\eta$ is still centred around zero (Fig. \ref{fig:post}, d). 
Thus, the analysis of the estimates of individual parameter 
values in the three models only supports a strong 
effect of chaining. Furthermore, the comparison of the models` fit 
favours the chaining model. In panel e of figure \ref{fig:post} 
we show the distribution of $pseudo-R^2$ calculated using 
samples from the posterior distributions shown before. 
Note, $pseudo-R^2$ can have negative values, which is when
the log-likelihood of the model is lower than that of a model that
triggers neutral preferences. Even though the peak of the three
$pseudo-R^2$ distributions were not very different, the model with only
chaining produced a distribution of $pseudo-R^2$ where more values
were positive (to the right of the black line in Fig. \ref{fig:post} f). This
shows that accounting for variation in the parameter estimates the
model with chaining gives a better fit to the data, despite
having one parameter less than the full model. We have not shown 
here the marginal posterior distributions of the scaling constant, 
given that they do not bring biological insight. 
Their visualizations can be found in the supplementary 
material (Fig. \ref{fig:scaConst}), as well as the diagnostics of the MCMCs
(Figs. \ref{fig:diagnosticsfull},\ref{fig:diaggam},\ref{fig:diagNeg}).



<!-- (ref:pred) Observed and predicted probability of choosing a visitor.  -->
<!-- Left-hand side panel: colour contour shows the prediction from the  -->
<!-- learning model using the mode of the posterior distributions of parameters  -->
<!-- recovered by the statistical analysis. Dots show the frequency of visitor  -->
<!-- choices for the 12 reef sites, as well as the corresponding relative  -->
<!-- cleaner fish abundance (x axis) and frequency of visitors clients leaving  -->
<!-- the cleaning station (y axis). Right-hand side panels: Variation of the  -->
<!-- predicted probabilities  of choosing a visitor over a resident and their  -->
<!-- observed values for 12 locations. Circles show the mean prediction for  -->
<!-- each location from 100 samples taken from the posterior distribution.  -->
<!-- Thick and thin bars show the 66 and 95% credible interval, respectively,  -->
<!-- taken from those posterior samples. Squares show the predictions used for the -->
<!-- panel on the right-hand side. Colour coding denotes different reef site/year  -->
<!-- of the data collection (see Empirical data section). The black line corresponds  -->
<!-- to a perfect match between observed and predicted probabilities. Upper  -->
<!-- panels (a and b) show predictions from a model including chaining and  -->
<!-- penalty; middle panels (c and d) from a  model with only chaining; lower  -->
<!-- panels (e and f) from a model with penalty only. -->

```{r pred, fig.cap='Observed and predicted probability of choosing a visitor. Left-hand side panel: colour contour shows the prediction from the learning model using the mode of the posterior distributions of parameters recovered by the statistical analysis. Dots show the frequency of visitor choices for the 12 reef sites, as well as the corresponding relative cleaner fish abundance (x axis) and frequency of visitors clients leaving the cleaning station (y axis). Right-hand side panels: Variation of the predicted probabilities  of choosing a visitor over a resident and their observed values for 12 locations. Circles show the mean prediction for each location from 100 samples taken from the posterior distribution. Thick and thin bars show the 66 and 95\\% credible interval, respectively, taken from those posterior samples. Squares show the predictions used for the panel on the right-hand side. Colour coding denotes different reef site/year of the data collection (see Empirical data section). The black line corresponds to a perfect match between observed and predicted probabilities. Upper panels (a and b) show predictions from a model including chaining and penalty; middle panels (c and d) from a  model with only chaining; lower panels (e and f) from a model with penalty only.',out.width='100%',fig.height=5,cache=FALSE}

 source(here("predictions.R"))
 ggarrange(cont.obs.pred.both,scatter.obs.pred.both,
 cont.obs.pred.gam,scatter.obs.pred.gam,
 cont.obs.pred.Nrew,scatter.obs.pred.Nrew,
           labels=c('a','b','c','d','e','f'),common.legend = F,
   heights=c(1.4,1,1),nrow = 3,ncol = 2)

```

The main reason for chaining and penalty to give different
predictions is the way that cleaner fish relative abundance influences the
preference for the visitor clients. Visitor leaving probability has a
similar positive effect on the probability of choosing the visitor
clients on all three models, they all predict an increase preference 
for visitors as the visitor probability increases  (Fig. \ref{fig:pred}).
In contrast, cleaner fish relative abundance has a different effect in the
model with only chaining, compared to the other two models,
full model and penalty. In the model with only chaining, only intermediate 
cleaner fish abundance triggers a preference for the visitor 
clients (Fig. \ref{fig:pred} c). In both models with penalty, 
both intermediate and low cleaner abundance triggered a preference 
for the visitor (Fig. \ref{fig:pred} a,e). Note, however, we calculated 
preferences shown in figure \ref{fig:pred} left panels by using only the mode 
of the posterior distributions, and by holding constant the balance between 
resident and visitors´ abundances. Panels on the right, show how close 
predictions are from the observed data, allowing the balance between 
client types to vary and using a set of samples from the posterior 
distribution. 


## Discussion

In this study, our main aim was to unravel which of two potential 
cognitive mechanisms, chaining of events, penalty, 
or their combination, best explains wild-caught cleaner fish 
performance in the ephemeral reward task, while accounting for their
ecological conditions. To evaluate the merits of each of these two mechanisms
separately and combined, we considered cleaner fish performance 
in the lab test to have its origin from the rule these fish applied 
in their natural environment. That is, individuals that 
solved the task already had a preference for 
visitors clients and generalized this rule to the lab conditions 
once being familiar with the task.

While all three models captured well the positive relationship between visitor 
leaving behaviour and cleaner fish performance in
the market task [@triki_Biological_2019], only the chaining mechanism 
predicted that cleaner fish performance in the task should be low in 
habitats with low cleaner-to-client ratios, regardless of the 
visitor leaving probability. In contrast, models including 
negative reward predicted the highest performance in the ephemeral 
reward task when relative cleaner fish abundance is low, particularly 
together with a high probability of visitor leaving (Fig. \ref{fig:pred}).
Low relative cleaner fish abundances mean the market has an excess of demand for
cleaning services. In the models, this translates to a cleaning station
that is frequently full. Thus, when a visitor leaves, it is likely that the
cleaner fish will have access to another client in the next step. Therefore, 
there will not be much difference in future reward between choosing 
a visitor and a resident, and cleaners will not develop a preference for the
visitor in these conditions. On the other hand, the effect of negative reward 
on cleaner fish preference is the opposite, as in a busy cleaning station, to
that of chaining. cleaner fish will get more often the resident-visitor
state and will develop a preference for the visitor faster. 
At high cleaner fish abundances, the resident-visitor state
becomes so rare that neither mechanism is very efficient 
at generating a preference for visitors. When facing the 
resident-visitor choice, it is still best to choose the visitor; however,
the learning machinery will not be able to develop this preference
efficiently. Overall, the models suggest that chaining is the cognitive
mechanism that allows cleaner fish to adaptively adjust to their 
biological market ecological conditions.

Previous research showed that cleaner fish living at high population 
densities and giving service priority to the visitor plate in the 
ephemeral reward task, as well as cleaner fish living at low 
densities but denying service priority to the visitor plate possess 
larger forebrains; a key teleost brain region associated with 
behavioural flexibility and social intelligence. Those failing to 
show optimized decision-rules given their local ecological 
conditions had relatively smaller forebrains  [@triki_Brain_2020]. 
Triki *et al.*  refer to the former as socially competent cleaner fish,
while the second group as socially incompetent cleaner fish. 
Social competence is the ability to optimise social behaviour 
to the available social information [@taborsky_Social_2012;
@bshary_Cooperation_2015;@varela_Correlated_2020]. Our analyses yielded no 
evidence that the difference in social competence with respect to the local 
ecological conditions and associated brain morphology, 
found by Triki *et al* [@triki_Brain_2020], is due to the mechanism 
used to incorporate future consequences. It is conceivable that 
high performing individuals from low population densities reef sites 
use negative reinforcement instead of chaining, but in that 
case, negative reinforcement should have explained at least 
part of the data. Configurational learning or 
chunking [@sutherland_Configural_1989;@miller_Magical_1956], 
the second component necessary to solve the ephemeral 
reward task [@quinones_Reinforcement_2019], was 
not varied in the models we analysed here. However, while chunking tendencies 
should vary to allow individuals to adapt to their local conditions 
[@prat_Modelling_2022;@kolodny_Evolution_2014], systematic differences 
in individual chunking tendencies would not explain how socially 
competent decisions vary as a function of relative abundance. 
Therefore, it remains currently unclear what cell-demanding 
mechanisms may cause variation in social competence that 
translates into site-specific variation in performance in the 
ephemeral reward task. 

Our models are inspired by the general processes of associative learning
where short term rewards are translated into decision making; thus, it 
ignores alternative channels of information that could be relevant in 
market-like situations. For example, the model does not investigate whether 
cleaner fish actually assess the frequency of client visits or a mean frequency 
of visitors leaving. The updating learning mechanism for the development 
of preferences works on a trial-by-trial basis. In the model, cleaner fish 
do not need to assess the actual state of the market, *i.e.* their abundance, 
the abundance of residents and visitors, and client visitation rate as an 
indicator of demand. They only need to assess the short-term 
consequences of their own decisions on food intake and chain them. 
Also, for the sake of simplicity, the model ignores 
the process by which cleaner fish discriminate 
residents and visitor clients. A model that accounts for this discrimination
probably would involve the development of preferences for morphological or 
behavioural features that are statistically associated with visitors 
or residents. For example, visitors are on average larger 
than residents in body size [@bshary_Cleaner_2001], and contrary to 
residents, they are less likely to chase a cleaner fish that fails to cooperate
and instead cheats its client by taking a bite of mucus 
[@bshary_Asymmetric_2002]. Given these associations, chaining might 
produce the decision-rule “choose the larger client and/or the less aggressive 
client”, which is not a useful rule in the standard ephemeral reward task. 


In conclusion, our study shows that variation in cognitive performance as 
a function of the local ecological conditions may set the stage for the use of 
mechanistic modelling to identify the cognitive processes underlying 
learning in animals. The combination overcomes the limitations 
of the general philosophy in animal cognition to apply the logic of 
Lloyd Morgen's canon (Occam's razor). Cognitive experiments with the 
aim of excluding basic reinforcement learning as a potential 
explanation (operant and/or classical conditioning) of performance 
often employ one trial experiments requiring animals to solve the 
task on the first possible occasion. 
For example, any theory of mind task needs to be solved in the first
trial in order to exclude fast conditioning [@heyes_Theory_1998].
Similarly, subjects need to solve a social learning task on the first
trial to accept imitation as a mechanism over stimulus/local
enhancement. Such strict conditions are virtually never met. For
example, potato washing by Japanese macaques, an iconic example of
social learning, took several years to spread within the group
[@kawamura_Process_1959], meaning that any learner had been repeatedly
exposed to demonstrations before acquisition. Importantly, Galef
[@galef_Question_1992] refuted imitation as a mechanism not simply because
of the repeated exposure but because a (rather qualitative) analysis of
the spread of potato washing across individuals did not follow the
prediction based on imitation learning (see also
[@hirata_SweetPotato_2001]). In our case, the number of trials it took
cleaners to learn the solution to the ephemeral rewards task 
would never allow excluding an important role of 
penalty based on the data
alone. However, fitting model predictions to our comprehensive empirical 
data set revealed that a more complex mechanism, estimation of future reward,
fits the data better.


\newpage

# Supplementary material


```{=tex}
\beginsupplement
```

<!-- (ref:rawdata)  -->


```{r rawdata,fig.cap='Relation between the response variable used in this study and the criteria used in previous studies to assess performance in the ephemeral reward task. In the x axis, we classified the performance of cleaner fish according to whether they developed a preference for the visitor in the initial round, in the initial and reversal, or none of them. In the y axis, we add the choices of two experimental sessions: panel on the left uses one session from the initial round and one from the reversal round when possible (as described in the main text); panel on the right uses two sessions from the initial round for all fish.',out.width='100%',fig.height=5,cache=FALSE}
plot_grid(initANDRev,init,ncol = 2)

```

<!-- (ref:scaConst) Posterior distributions for scaling constant for the three models -->
<!-- ("Full model", chaining only" and "penalty only"). We show the kernel -->
<!-- density estimates, below the mode (black dot) and the 65% (light blue shade) -->
<!-- and 95% (grey shade)  highest posterior density interval. On the top, -->
<!-- panel a shows the posterior distribution from the full model; panel b from -->
<!-- the model with only chaining; and panel c from a model with only penalty. -->

```{r scaConst, fig.cap='Posterior distributions for scaling constant for the three models (Full model, chaining only and penalty only). We show the kernel density estimates, below the mode (black dot) and the 65\\% (light blue shade) and 95\\% (grey shade)  highest posterior density interval. On the top, panel a shows the posterior distribution from the full model; panel b from the model with only chaining; and panel c from a model with only penalty.',fig.pos = 'H', out.extra = "",out.width='95%',fig.height=5}

sca.both.post<-ggplot(data=MCMCdata1,aes(x=scaleConst)) + 
  stat_halfeye(aes(fill=stat(cut(x,breaks = cuts.1$scaleConst))),
               point_interval = mode_hdi, .width = c(.66, .95),
               point_size=3,show.legend=FALSE) + 
  labs(title=labelsPlot.Scen[1],x="",y="")+
  theme_classic()+
  scale_fill_manual(values=c("gray85","skyblue","gray85"))
sca.gam.post<-ggplot(data=MCMCdata2,aes(x=scaleConst)) + 
  stat_halfeye(aes(fill=stat(cut(x,breaks = cuts.2$scaleConst))),
               point_interval = mode_hdi, 
               .width = c(.66, .95),point_size=3,
               show.legend=FALSE)+theme_classic()+
  scale_fill_manual(values=c("gray85","skyblue","gray85"))+
  labs(title=labelsPlot.Scen[2],x="",y="")
sca.nRew.post<-ggplot(data=MCMCdata3,aes(x=scaleConst)) +
  stat_halfeye(aes(fill=stat(cut(x,breaks = cuts.3$scaleConst[2:4]))),
               point_interval = mode_hdi, .width = c(.66, .95),
               point_size=3,
               show.legend=FALSE) +
  scale_fill_manual(values=c("skyblue","gray85"))+
  labs(title=labelsPlot.Scen[3],x="",y="")+
  theme_classic()
ggarrange(sca.both.post,sca.gam.post,
          sca.nRew.post,
          labels=c('a','b','c'),common.legend = F)

```

<!-- (ref:diagnosticsfull) MCMC convergence diagnostics for the full model.  -->
<!-- On the left trace-plots, on the right changes along the chain of the  -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998]. -->


```{r diagnosticsfull, fig.cap='MCMC convergence diagnostics for the full model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor.',out.width='100%',fig.height=5,cache=FALSE,message=FALSE}

longMCMCdata1<-melt(MCMCdata1,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","negReward","scaleConst"))
                    
parlab<-list(`gamma` =expression(gamma),
          `negReward`=expression(eta),
          `scaleConst`="Scaling constant")

traces.full<-ggplot(longMCMCdata1,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = function(variable,value){
  return(parlab[value])})+
  theme_classic()+
  theme(legend.position = 'top')

gel.full<-as.data.table(gelman.diag(mcmcList.1)$psrf)
gel.full[,parameter:=c("gamma","negReward","scalConst")]

pdf(file = NULL)
gp.dat<-gelman.plot(mcmcList.1)
rubbish <- dev.off()

gp.dat.frame <- data.table(rbind(as.data.frame(gp.dat[["shrink"]][,,1]), 
                          as.data.frame(gp.dat[["shrink"]][,,2])), 
                q=rep(dimnames(gp.dat[["shrink"]])[[3]], each=nrow(gp.dat[["shrink"]][,,1])),
                last.iter=rep(gp.dat[["last.iter"]], length(gp.dat)))

gelman.full<-ggplot(melt(gp.dat.frame, c("q","last.iter"), value.name="shrink_factor"), 
       aes(last.iter, shrink_factor, colour=q, linetype=q)) + 
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y  = element_blank())
  
ggarrange(traces.full,gelman.full)

```

<!-- (ref:diagnosticsGam) MCMC convergence diagnostics for the chaining model.  -->
<!-- On the left trace-plots, on the right changes along the chain of the  -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998] -->

```{r diaggam,fig.cap='MCMC convergence diagnostics for the chaining model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor ',echo=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata2<-melt(MCMCdata2,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("gamma","scaleConst"))
                    
parlab<-list(`gamma` =expression(gamma),
          `scaleConst`="Scaling constant")

traces.gam<-ggplot(longMCMCdata2,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = function(variable,value){
  return(parlab[value])})+
  theme_classic()+
  theme(legend.position = 'top')

gel.gam<-as.data.table(gelman.diag(mcmcList.2)$psrf)
gel.gam[,parameter:=c("gamma","scalConst")]

pdf(file = NULL)
gp.dat.gam<-gelman.plot(mcmcList.2)
rubbish <- dev.off()

gp.dat.frame.gam <- data.table(rbind(as.data.frame(gp.dat.gam[["shrink"]][,,1]), 
                          as.data.frame(gp.dat.gam[["shrink"]][,,2])), 
                q=rep(dimnames(gp.dat.gam[["shrink"]])[[3]], each=nrow(gp.dat.gam[["shrink"]][,,1])),
                last.iter=rep(gp.dat.gam[["last.iter"]], length(gp.dat.gam)))

gelman.gam<-ggplot(melt(gp.dat.frame.gam, c("q","last.iter"), value.name="shrink_factor"), 
       aes(last.iter, shrink_factor, colour=q, linetype=q)) + 
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.gam,gelman.gam)
```

<!-- (ref:diagNeg) MCMC convergence diagnostics for the penalty model. -->
<!-- On the left trace-plots, on the right changes along the chain of the -->
<!-- Gelman and Rubin's shrink factor [@brooks_General_1998] -->

```{r diagNeg,fig.cap='MCMC convergence diagnostics for the penalty model. On the left trace-plots, on the right changes along the chain of the Gelman and Rubin shrink factor',out.width='100%',fig.height=5,cache=FALSE,message=FALSE,out.width='100%',fig.height=5,cache=FALSE,message=FALSE}
longMCMCdata3<-melt(MCMCdata3,id.vars = c("iteration","seed"),
                    variable.name = "parameter",
                    measure.vars = c("negReward","scaleConst"))

parlab<-list(`negReward` =expression(eta),
          `scaleConst`="Scaling constant")

traces.neg<-ggplot(longMCMCdata3,aes(x=iteration,y=value,colour=as.factor(seed)))+
  geom_line()+
  scale_color_manual(values = multDiscrPallet)+
  labs(color="Chain")+
  facet_grid(rows=vars(parameter),scales = "free",switch = "y",
             labeller = function(variable,value){
  return(parlab[value])})+
  theme_classic()+
  theme(legend.position = 'top')


pdf(file = NULL)
gp.dat.neg<-gelman.plot(mcmcList.3)
rubbish <- dev.off()

gp.dat.frame.neg <- data.table(rbind(as.data.frame(gp.dat.neg[["shrink"]][,,1]),
                          as.data.frame(gp.dat.neg[["shrink"]][,,2])),
                q=rep(dimnames(gp.dat.neg[["shrink"]])[[3]], each=nrow(gp.dat.neg[["shrink"]][,,1])),
                last.iter=rep(gp.dat.neg[["last.iter"]], length(gp.dat.neg)))

gelman.neg<-ggplot(melt(gp.dat.frame.neg, c("q","last.iter"), value.name="shrink_factor"),
       aes(last.iter, shrink_factor, colour=q, linetype=q)) +
  geom_hline(yintercept=1, colour="grey30", lwd=0.2) +
  geom_line() +
  facet_grid(rows = vars(variable)) +
  labs(x="Last Iteration in Chain", y="Shrink Factor",
       colour="Quantile", linetype="Quantile") +
  scale_linetype_manual(values=c(2,1))+
  theme_classic()+
  theme(legend.position = 'top',strip.text.y = element_blank())

ggarrange(traces.neg,gelman.neg)
```

```{r param , echo=FALSE,results='asis'}
  cat(' Table: \\label{tab:param} Parameter values with which the model was run 
  in the MCMC. $\\sigma$ refers to the amplitude of the perturbation kernel with the subscript indicating the associated parameter. New values were taken from a uniform distribution. $\\alpha$ refers to the learning rate. 

  | Parameter | Value |
|----------:|:-----:|
| Learning rounds | 10000 |
| Reward value | 1 |
| $\\alpha$ | 0.05 |
| $\\sigma_{\\gamma}$ | 0.3 |
| $\\sigma_{\\eta}$ | 4 |
| $\\sigma_{Sca.Const.}$ | 300 |
| Number of chains | 5 |
| Chain length | $1^5$ |
')

```


# References {#references .unnumbered}